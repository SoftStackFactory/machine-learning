{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"05.02b.linear-regression-regularization-with-multiple-models.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"2mnArdogxOjj","colab_type":"text"},"source":["# Linear Regression With Multiple Models\n","\n","We will learn how to solve a linear regression problem through the use of multiple linear regression model algorithms, specifically regularization algorithms. In conjunction we will perform a cross validation grid search, looping through different models to find the model and it's hyper-parameter set with the highest perforformance based on the current state of the data. We will use this model to predict the transaction price of a house using a real estate dataset. \n","\n","## Overview: \n","1. Import the dataset\n","2. Separate the dataset's features from target variable\n","3. Split data into training and testing sets\n","4. Make a dictionary called `pipelines_dict` to store multiple pipelines, one for each algorithm \n","5. Make multiple hyper-parameter dictionaries, one for each algorithm\n","6. Make a dictionary called `hyper_parameters_dict` to hold all hyper-parameter dictionaries\n","7. Make a dictionary called `best_performing_models_dict` to populate and store the highest perfoming models during the cross validation grid search loop \n","8. Create a loop to perform multiple cross validaton grid searches, one for each algorithm using the training set. Setting the following parameters of the `GridSearchCV` object initialization:\n","  - `estimator` parameter will be assigned with one of the `pipeline` object instances with respect to a specific algorithm selected for the current loop iteration as its argument\n","  - `param_grid` parameter will be assigned with one of the `hyper-parameter-dictionary` instances with respect to a specific algorithm selected for the current loop iteration as its argument\n","9. Identify the highest performing algorithm with its hyper-parameters set using the training set\n","10. Evaluate the best performing model using the testing data\n","11. Using the best performing model predict values and compare those results to the real values\n","<hr>\n","\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"UAUVYQ7uUaDz","colab_type":"text"},"source":["## Import Required Libraries\n","\n","**Note:** You can tell the difference between a class and a function by the case sensivity. \n","\n","- A **class** will be captialized\n","- A **function** will be lowercase\n","- A **method**, or a function belonging to a class, will also be lowercase. You can call a method by invoking it through an instance of a class (instance method), or through a class definition (static method)\n","\n","References: \n","- [Understanding what a class is](https://www.hackerearth.com/practice/python/object-oriented-programming/classes-and-objects-i/tutorial/)\n","- [Differences between functions and methods](https://www.tutorialspoint.com/difference-between-method-and-function-in-python)\n","- [Different types of methods](https://www.bogotobogo.com/python/python_differences_between_static_method_and_class_method_instance_method.php)"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"EmHTYzZJxOjl","colab_type":"code","colab":{}},"source":["# Collection libraries \n","import numpy as np\n","import pandas as pd\n","\n","# Visual libraries \n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Helper for splitting training and testing data\n","from sklearn.model_selection import train_test_split\n","\n","# Models/Estimators\n","from sklearn.linear_model import LinearRegression\n","from sklearn.linear_model import Lasso\n","from sklearn.linear_model import Ridge\n","from sklearn.linear_model import ElasticNet\n","\n","# Helper for pipelines\n","from sklearn.pipeline import make_pipeline\n","\n","# Helper for normailizing dataset\n","from sklearn.preprocessing import StandardScaler\n","\n","# Helper for cross-validation\n","from sklearn.model_selection import GridSearchCV"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"CZ0OZZFB8kBM"},"source":["#### Notes about imports with this notebook:\n","We will re-import some of the libraries when we use these modules, this is to get you used to importing and understanding their classes and functions. Reference the documentation to understand the libraries classes, methods, and functions. "]},{"cell_type":"markdown","metadata":{"id":"ny-A-wdQ7Yov","colab_type":"text"},"source":["## Load Data"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Es-ceD1D3ytV"},"source":["<hr>\n","\n","##### Mount Drive - **Google Colab Only Step**\n","\n","When using google colab in order to access files on our google drive we need to mount the drive by running the below python cell, then clicking the link it generates and pasting the code in the cell.\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"YKHWrhnJ3ytR","colab":{"base_uri":"https://localhost:8080/","height":33},"outputId":"8cfa0fa8-525e-4212-f7cd-7d2df551e815","executionInfo":{"status":"ok","timestamp":1576878621940,"user_tz":480,"elapsed":2593,"user":{"displayName":"Matt Hess","photoUrl":"","userId":"00800340733434721825"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":122,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"x5AzSk7g3ytO"},"source":["Change Directory To Access The Dependent Files - **Google Colab Only Step**"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"j9rgWUe03ytL","colab":{"base_uri":"https://localhost:8080/","height":50},"outputId":"b70a84d2-ed85-4b22-895a-cddc570e4998","executionInfo":{"status":"ok","timestamp":1576878621942,"user_tz":480,"elapsed":2535,"user":{"displayName":"Matt Hess","photoUrl":"","userId":"00800340733434721825"}}},"source":["directory = \"studsent\"\n","if (directory == \"student\"):\n","  %cd drive/Colab\\ Notebooks/machine-learning/\n","else:\n","  %cd drive/Shared\\ drives/Rubrik/Data\\ Science\\ Track/machine-learning"],"execution_count":123,"outputs":[{"output_type":"stream","text":["[Errno 2] No such file or directory: 'drive/Shared drives/Rubrik/Data Science Track/machine-learning'\n","/content/drive/Shared drives/Rubrik/Data Science Track/machine-learning\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YmJlqpSrhYt0","colab_type":"text"},"source":["<hr> \n","<br>\n","\n","### Import Real Estate Dataset\n","Read in the real estate dataset using the path provided and store it in a variable called `df`.\n","\n","#### Import the cleaned real estate dataset\n","- Use pandas' `read_csv` function\n","\n","#### Pandas' `read_csv` parameters:\n","- `filepath_or_buffer` (string): path of csv to import\n","\n","```python \n","filepath_or_buffer = './data/cleaned_and_feature_engineered_real_estate.csv'\n","```"]},{"cell_type":"code","metadata":{"id":"eQvU4eCwh7GJ","colab_type":"code","colab":{}},"source":["df = pd.read_csv(filepath_or_buffer = './data/cleaned_and_feature_engineered_real_estate.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HZ1veSMBXdiZ","colab_type":"text"},"source":["### Show Head Of Datset"]},{"cell_type":"code","metadata":{"id":"xs4zgyBMXiCG","colab_type":"code","outputId":"228b1079-fbcd-4c21-b942-cc83b219c5cb","executionInfo":{"status":"ok","timestamp":1576878621945,"user_tz":480,"elapsed":2320,"user":{"displayName":"Matt Hess","photoUrl":"","userId":"00800340733434721825"}},"colab":{"base_uri":"https://localhost:8080/","height":227}},"source":["df.head()"],"execution_count":125,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tx_price</th>\n","      <th>beds</th>\n","      <th>baths</th>\n","      <th>sqft</th>\n","      <th>year_built</th>\n","      <th>lot_size</th>\n","      <th>basement</th>\n","      <th>median_age</th>\n","      <th>married</th>\n","      <th>college_grad</th>\n","      <th>property_tax</th>\n","      <th>insurance</th>\n","      <th>median_school</th>\n","      <th>num_schools</th>\n","      <th>tx_year</th>\n","      <th>lifestyle_avg</th>\n","      <th>two_and_two</th>\n","      <th>exterior_walls_Brick</th>\n","      <th>exterior_walls_Brick veneer</th>\n","      <th>exterior_walls_Combination</th>\n","      <th>exterior_walls_Metal</th>\n","      <th>exterior_walls_Other</th>\n","      <th>exterior_walls_Siding (Alum/Vinyl)</th>\n","      <th>exterior_walls_Wood</th>\n","      <th>exterior_walls_missing</th>\n","      <th>roof_Asphalt</th>\n","      <th>roof_Composition Shingle</th>\n","      <th>roof_Other</th>\n","      <th>roof_Shake Shingle</th>\n","      <th>roof_missing</th>\n","      <th>property_type_Apartment / Condo / Townhouse</th>\n","      <th>property_type_Single-Family</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>295850.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>584.0</td>\n","      <td>2013.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>33.0</td>\n","      <td>65.0</td>\n","      <td>84.0</td>\n","      <td>234.0</td>\n","      <td>81.0</td>\n","      <td>9.0</td>\n","      <td>3.0</td>\n","      <td>2013.0</td>\n","      <td>1.493259</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>216500.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>612.0</td>\n","      <td>1965.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>39.0</td>\n","      <td>73.0</td>\n","      <td>69.0</td>\n","      <td>169.0</td>\n","      <td>51.0</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>2006.0</td>\n","      <td>0.676598</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>279900.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>615.0</td>\n","      <td>1963.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>28.0</td>\n","      <td>15.0</td>\n","      <td>86.0</td>\n","      <td>216.0</td>\n","      <td>74.0</td>\n","      <td>8.0</td>\n","      <td>3.0</td>\n","      <td>2012.0</td>\n","      <td>2.298254</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>379900.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>618.0</td>\n","      <td>2000.0</td>\n","      <td>33541.0</td>\n","      <td>0.0</td>\n","      <td>36.0</td>\n","      <td>25.0</td>\n","      <td>91.0</td>\n","      <td>265.0</td>\n","      <td>92.0</td>\n","      <td>9.0</td>\n","      <td>3.0</td>\n","      <td>2005.0</td>\n","      <td>2.473650</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>340000.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>634.0</td>\n","      <td>1992.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>37.0</td>\n","      <td>20.0</td>\n","      <td>75.0</td>\n","      <td>88.0</td>\n","      <td>30.0</td>\n","      <td>9.0</td>\n","      <td>3.0</td>\n","      <td>2002.0</td>\n","      <td>1.661371</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   tx_price  ...  property_type_Single-Family\n","0  295850.0  ...                            0\n","1  216500.0  ...                            0\n","2  279900.0  ...                            0\n","3  379900.0  ...                            0\n","4  340000.0  ...                            0\n","\n","[5 rows x 32 columns]"]},"metadata":{"tags":[]},"execution_count":125}]},{"cell_type":"markdown","metadata":{"id":"0BFt6HvMXlQH","colab_type":"text"},"source":["### Show Tail Of Dataset"]},{"cell_type":"code","metadata":{"id":"8lXDeQwAXpWU","colab_type":"code","outputId":"f295c260-1fc7-482c-a7f3-ceecc9008ca0","executionInfo":{"status":"ok","timestamp":1576878621946,"user_tz":480,"elapsed":2180,"user":{"displayName":"Matt Hess","photoUrl":"","userId":"00800340733434721825"}},"colab":{"base_uri":"https://localhost:8080/","height":227}},"source":["df.tail()"],"execution_count":126,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tx_price</th>\n","      <th>beds</th>\n","      <th>baths</th>\n","      <th>sqft</th>\n","      <th>year_built</th>\n","      <th>lot_size</th>\n","      <th>basement</th>\n","      <th>median_age</th>\n","      <th>married</th>\n","      <th>college_grad</th>\n","      <th>property_tax</th>\n","      <th>insurance</th>\n","      <th>median_school</th>\n","      <th>num_schools</th>\n","      <th>tx_year</th>\n","      <th>lifestyle_avg</th>\n","      <th>two_and_two</th>\n","      <th>exterior_walls_Brick</th>\n","      <th>exterior_walls_Brick veneer</th>\n","      <th>exterior_walls_Combination</th>\n","      <th>exterior_walls_Metal</th>\n","      <th>exterior_walls_Other</th>\n","      <th>exterior_walls_Siding (Alum/Vinyl)</th>\n","      <th>exterior_walls_Wood</th>\n","      <th>exterior_walls_missing</th>\n","      <th>roof_Asphalt</th>\n","      <th>roof_Composition Shingle</th>\n","      <th>roof_Other</th>\n","      <th>roof_Shake Shingle</th>\n","      <th>roof_missing</th>\n","      <th>property_type_Apartment / Condo / Townhouse</th>\n","      <th>property_type_Single-Family</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1877</th>\n","      <td>385000.0</td>\n","      <td>5.0</td>\n","      <td>6.0</td>\n","      <td>6381.0</td>\n","      <td>2004.0</td>\n","      <td>224334.0</td>\n","      <td>1.0</td>\n","      <td>46.0</td>\n","      <td>76.0</td>\n","      <td>87.0</td>\n","      <td>1250.0</td>\n","      <td>381.0</td>\n","      <td>10.0</td>\n","      <td>3.0</td>\n","      <td>2002.0</td>\n","      <td>-0.792553</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1878</th>\n","      <td>690000.0</td>\n","      <td>5.0</td>\n","      <td>6.0</td>\n","      <td>6501.0</td>\n","      <td>1956.0</td>\n","      <td>23086.0</td>\n","      <td>1.0</td>\n","      <td>42.0</td>\n","      <td>73.0</td>\n","      <td>61.0</td>\n","      <td>1553.0</td>\n","      <td>473.0</td>\n","      <td>9.0</td>\n","      <td>3.0</td>\n","      <td>2015.0</td>\n","      <td>0.247411</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1879</th>\n","      <td>600000.0</td>\n","      <td>5.0</td>\n","      <td>6.0</td>\n","      <td>7064.0</td>\n","      <td>1995.0</td>\n","      <td>217800.0</td>\n","      <td>1.0</td>\n","      <td>43.0</td>\n","      <td>87.0</td>\n","      <td>66.0</td>\n","      <td>942.0</td>\n","      <td>287.0</td>\n","      <td>8.0</td>\n","      <td>1.0</td>\n","      <td>1999.0</td>\n","      <td>-0.643123</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1880</th>\n","      <td>759900.0</td>\n","      <td>5.0</td>\n","      <td>6.0</td>\n","      <td>7500.0</td>\n","      <td>2006.0</td>\n","      <td>8886.0</td>\n","      <td>1.0</td>\n","      <td>43.0</td>\n","      <td>61.0</td>\n","      <td>51.0</td>\n","      <td>803.0</td>\n","      <td>245.0</td>\n","      <td>5.0</td>\n","      <td>2.0</td>\n","      <td>2009.0</td>\n","      <td>-0.524305</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1881</th>\n","      <td>735000.0</td>\n","      <td>5.0</td>\n","      <td>6.0</td>\n","      <td>7515.0</td>\n","      <td>1958.0</td>\n","      <td>10497.0</td>\n","      <td>1.0</td>\n","      <td>37.0</td>\n","      <td>80.0</td>\n","      <td>86.0</td>\n","      <td>1459.0</td>\n","      <td>444.0</td>\n","      <td>9.0</td>\n","      <td>3.0</td>\n","      <td>2015.0</td>\n","      <td>-0.696683</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      tx_price  ...  property_type_Single-Family\n","1877  385000.0  ...                            1\n","1878  690000.0  ...                            1\n","1879  600000.0  ...                            1\n","1880  759900.0  ...                            1\n","1881  735000.0  ...                            1\n","\n","[5 rows x 32 columns]"]},"metadata":{"tags":[]},"execution_count":126}]},{"cell_type":"markdown","metadata":{"id":"X7kVrx-1WItT","colab_type":"text"},"source":["<hr> \n","\n","<br>\n","\n","## Separate the dataset's features from the target variable\n","\n","**Tasks:**\n","- Print shape of original DataFrame before manipulating the DataFrame\n","- Create a new DataFrame called `X` to contain only the features \n","- Create a new DataFrame called `y` to contain only the labels\n","\n","<br>\n","\n","### Question: \n","Why would you split the data this way?\n","\n"]},{"cell_type":"markdown","metadata":{"id":"haWxJNQqRcnB","colab_type":"text"},"source":["Answer:\n","\n","We will do this to separate the features of the dataset from the target value. For our problem we will set the `tx_price` as the target variable for this machine learning model, because we want to predict the house price based on a selected amount of other features of the dataset.     "]},{"cell_type":"markdown","metadata":{"id":"49c9QRT9RcOn","colab_type":"text"},"source":["<br>\n","\n","### Print Shape Of Original DataFrame\n","We will do this to confirm our manipulations later\n"]},{"cell_type":"code","metadata":{"id":"fEYW_FT5XCW3","colab_type":"code","outputId":"0ed65152-b111-4c08-bad1-45e4d8943f16","executionInfo":{"status":"ok","timestamp":1576878621950,"user_tz":480,"elapsed":2126,"user":{"displayName":"Matt Hess","photoUrl":"","userId":"00800340733434721825"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["df.shape"],"execution_count":127,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1882, 32)"]},"metadata":{"tags":[]},"execution_count":127}]},{"cell_type":"markdown","metadata":{"id":"s6mLVXRDXV-E","colab_type":"text"},"source":["### Create A DataFrame Called `X` To Hold All The Features\n","**Note:** `X` is uppercase because it's a 2D array / matrix. A matrix can hold multiple rows and more than one column. \n","\n","**Tip:** Consider using the DataFrame's `drop` method to create this new DataFrame\n","\n","\n","#### DataFrame's `drop` method parameters:\n","- `labels` (string or list of strings): index or a  column labels to drop\n","- `axis`  ({0 or ‘index’, 1 or ‘columns’}): default 0; whether to drop labels from the index (0 or ‘index’) or columns (1 or ‘columns’)\n","- `inplace` (bool): default False; If True, do operation inplace and return None.\n","\n"]},{"cell_type":"code","metadata":{"id":"jHCpurRhYoi9","colab_type":"code","colab":{}},"source":["X = df.drop(labels=\"tx_price\", axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WXsA6YQLcB4c","colab_type":"text"},"source":["#### Show Shape of `X` to make sure we created the features DataFrame correctly:\n","\n","It should have 1882 rows and 31 columns"]},{"cell_type":"code","metadata":{"id":"j401z0aVZShG","colab_type":"code","outputId":"a1fbff34-a1f8-400a-e362-3663d4755452","executionInfo":{"status":"ok","timestamp":1576878621953,"user_tz":480,"elapsed":2040,"user":{"displayName":"Matt Hess","photoUrl":"","userId":"00800340733434721825"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["X.shape"],"execution_count":129,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1882, 31)"]},"metadata":{"tags":[]},"execution_count":129}]},{"cell_type":"markdown","metadata":{"id":"a_Ou4ZIpaK-L","colab_type":"text"},"source":["### Print Head Of Features Matrix `X`"]},{"cell_type":"code","metadata":{"id":"FAnSrTc3aNuN","colab_type":"code","outputId":"e738295a-e34e-4541-e439-5ac4643572f7","executionInfo":{"status":"ok","timestamp":1576878622393,"user_tz":480,"elapsed":2401,"user":{"displayName":"Matt Hess","photoUrl":"","userId":"00800340733434721825"}},"colab":{"base_uri":"https://localhost:8080/","height":227}},"source":["X.head()"],"execution_count":130,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>beds</th>\n","      <th>baths</th>\n","      <th>sqft</th>\n","      <th>year_built</th>\n","      <th>lot_size</th>\n","      <th>basement</th>\n","      <th>median_age</th>\n","      <th>married</th>\n","      <th>college_grad</th>\n","      <th>property_tax</th>\n","      <th>insurance</th>\n","      <th>median_school</th>\n","      <th>num_schools</th>\n","      <th>tx_year</th>\n","      <th>lifestyle_avg</th>\n","      <th>two_and_two</th>\n","      <th>exterior_walls_Brick</th>\n","      <th>exterior_walls_Brick veneer</th>\n","      <th>exterior_walls_Combination</th>\n","      <th>exterior_walls_Metal</th>\n","      <th>exterior_walls_Other</th>\n","      <th>exterior_walls_Siding (Alum/Vinyl)</th>\n","      <th>exterior_walls_Wood</th>\n","      <th>exterior_walls_missing</th>\n","      <th>roof_Asphalt</th>\n","      <th>roof_Composition Shingle</th>\n","      <th>roof_Other</th>\n","      <th>roof_Shake Shingle</th>\n","      <th>roof_missing</th>\n","      <th>property_type_Apartment / Condo / Townhouse</th>\n","      <th>property_type_Single-Family</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>584.0</td>\n","      <td>2013.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>33.0</td>\n","      <td>65.0</td>\n","      <td>84.0</td>\n","      <td>234.0</td>\n","      <td>81.0</td>\n","      <td>9.0</td>\n","      <td>3.0</td>\n","      <td>2013.0</td>\n","      <td>1.493259</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>612.0</td>\n","      <td>1965.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>39.0</td>\n","      <td>73.0</td>\n","      <td>69.0</td>\n","      <td>169.0</td>\n","      <td>51.0</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>2006.0</td>\n","      <td>0.676598</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>615.0</td>\n","      <td>1963.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>28.0</td>\n","      <td>15.0</td>\n","      <td>86.0</td>\n","      <td>216.0</td>\n","      <td>74.0</td>\n","      <td>8.0</td>\n","      <td>3.0</td>\n","      <td>2012.0</td>\n","      <td>2.298254</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>618.0</td>\n","      <td>2000.0</td>\n","      <td>33541.0</td>\n","      <td>0.0</td>\n","      <td>36.0</td>\n","      <td>25.0</td>\n","      <td>91.0</td>\n","      <td>265.0</td>\n","      <td>92.0</td>\n","      <td>9.0</td>\n","      <td>3.0</td>\n","      <td>2005.0</td>\n","      <td>2.473650</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>634.0</td>\n","      <td>1992.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>37.0</td>\n","      <td>20.0</td>\n","      <td>75.0</td>\n","      <td>88.0</td>\n","      <td>30.0</td>\n","      <td>9.0</td>\n","      <td>3.0</td>\n","      <td>2002.0</td>\n","      <td>1.661371</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   beds  ...  property_type_Single-Family\n","0   1.0  ...                            0\n","1   1.0  ...                            0\n","2   1.0  ...                            0\n","3   1.0  ...                            0\n","4   1.0  ...                            0\n","\n","[5 rows x 31 columns]"]},"metadata":{"tags":[]},"execution_count":130}]},{"cell_type":"markdown","metadata":{"id":"L5tE9Jx7YkEQ","colab_type":"text"},"source":["### Create A Series Called `y` To Hold All The Labels\n","**Note:** `y` is lowercase because it's Series, meaning can hold multiple rows with only one column per row.\n","\n"]},{"cell_type":"code","metadata":{"id":"lqhOSR5XY1QA","colab_type":"code","colab":{}},"source":["y = df.loc[:, 'tx_price']"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GJaGIwlIZWbJ","colab_type":"text"},"source":["#### Show Shape of `y` to make sure we created the target series correctly:\n","\n","It should have 1882 rows and 1 column\n","\n","**Note:** the shape will print out like this, `(1882,)` which means that is has 1882 rows and 1 column."]},{"cell_type":"code","metadata":{"id":"VYJ3UxBxbz8b","colab_type":"code","outputId":"f14f2884-5da4-4682-87bf-3467e8c6ed99","executionInfo":{"status":"ok","timestamp":1576878622395,"user_tz":480,"elapsed":2116,"user":{"displayName":"Matt Hess","photoUrl":"","userId":"00800340733434721825"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["y.shape"],"execution_count":132,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1882,)"]},"metadata":{"tags":[]},"execution_count":132}]},{"cell_type":"markdown","metadata":{"id":"b4rnN6U0b4ug","colab_type":"text"},"source":["### Print Head Of Label Series `y`"]},{"cell_type":"code","metadata":{"id":"Fk_qjzB2aIho","colab_type":"code","outputId":"57aa01b9-c9a6-4ecf-9063-512fad9cc5aa","executionInfo":{"status":"ok","timestamp":1576878622396,"user_tz":480,"elapsed":2086,"user":{"displayName":"Matt Hess","photoUrl":"","userId":"00800340733434721825"}},"colab":{"base_uri":"https://localhost:8080/","height":115}},"source":["y.head()"],"execution_count":133,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    295850.0\n","1    216500.0\n","2    279900.0\n","3    379900.0\n","4    340000.0\n","Name: tx_price, dtype: float64"]},"metadata":{"tags":[]},"execution_count":133}]},{"cell_type":"markdown","metadata":{"id":"EFKzxqnr7uzd","colab_type":"text"},"source":["<hr>\n","<br>\n","\n","## Split Data Into Training And Testing \n","\n","Even though we will perform cross validation in the near future we will still want to split the dataset in to a training and testing set. We do this so that after we find the best estimator, or best fitted model, through utilizing the training data with a specific hyper-parameters values, we can evaluate the model using unseen testing data. This will allow us to understand if the model is overfitting or underfitting. \n","\n","Additional Resources:\n","- [Learn more about overfitting and underfitting](https://github.com/SoftStackFactory/PythonDataScienceHandbook/blob/master/notebooks/05.03-Hyperparameters-and-Model-Validation.ipynb)\n","- [Interested in how to better fit a model?](https://github.com/SoftStackFactory/PythonDataScienceHandbook/blob/master/notebooks/05.04-Feature-Engineering.ipynb)\n","\n","**Note:** The second resource is very informative about feature engineering, but we specifically want to emphasize the **Derived Features** section. \n"]},{"cell_type":"markdown","metadata":{"id":"2hoPli0Ug2H3","colab_type":"text"},"source":["#### Split Data Into Training And Testing Sets Using `train_test_split` function\n"," \n","Requirements: \n","- pass in `0.20` as the argument for the `test_size` parameter\n","- pass in `1` as the argument for the `random_state` parameter\n","\n","**Note:** We set the `random_state` parameter to a unique argument value so that when we run this notebook multiple times or using different computers we will recieve the same split of data, which is important for re-running experiments and simulations.\n","\n","[`train_test_split` function documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"k-sccN-mtTGl"},"source":["### Import `train_test_split` Function From Sklearn's Library"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"F3Js4vUKtTGo","colab":{}},"source":["# Helper for splitting training and testing sets\n","from sklearn.model_selection import train_test_split"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"llMrZ0-e-DX5","colab_type":"code","colab":{}},"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8SdNSJn0SqaG","colab_type":"text"},"source":["<hr>\n","\n","<br>\n","\n","## Cross Validation Recap\n","One disadvantage of using a holdout set, or in other words a static split of data for model validation is that we have lost a portion of our data to the model training. In this case some of the dataset would not contribute to the training of the model, saying we don't use cross validation and stick with splitting the data only once. This is not optimal, and can cause problems – especially if the initial set of training data is small.\n","\n","We will be using cross validation in conjunction to splitting the data using the `train_test_spit` function. We will do this so that we are not training and testing our model's evaluation with the same data, meaning we will eventually want to have our model get scored using unseen data to get a feel for how the model is performing.\n","\n","<br>\n","\n","### Perform A Cross Validation Grid Search \n","A Grid Search Cross Validation is an exhaustive search over specified hyper-parameter values to find the most performant estimator with a specific hyper-parameter set. \n","\n","<br>\n","\n","#### To perform a cross validation grid search on each machine learning algorithm we need to construct the following:\n","- Pipeline object, one for each algorithm \n","- Model hyper-parameter dictionary, one for each algorithm\n","\n","<br>\n","\n","Think of a Pipeline object as production line of transforming `features`, or X DataFrame, before eventually fitting the model. It's important to know that each argument of the pipeline, each transformer object, that will transform the `features` data, must have the following methods implemented:\n","- `fit()` \n","- `fit_transform()`\n","\n","<br>\n","\n","In summary the pipeline will pre-process any `features` data provided to it before fitting the model. We will accomplish this by calling the first parameter's `fit_transform` method. The output of the first object's `fit_transform` method will be passed automatically to the next parameter's `fit_transform` method, and so on. Eventually the output of the last transformer object's `fit_transform` method will be passed to the `fit()` method of the estimator object.\n","\n","#### Rundown of what is conceptually happening:\n","\n","``` python \n","def process_of_pipeline(self, Features, labels):\n","  Features_transformed = Feaures\n","  for name, transformer_object: in self.steps[:-1]:\n","    Features_transformed = transformer_object.fit_transform(Features_transformed, labels)\n","  estimator = self.steps[-1][1]\n","  estimator.fit(Features_transformed, labels) # The last step is the estimator, fit the model after all the transformation operations are complete\n","```\n","\n","**Note:** `Features` is capitalized because it's a 2D Matrix, which can interchangably be refered to as a DataFrame, or many rows of data each containing multiple columns of data points. While `labels` should be thought of as a Series, because each row only has one column to hold a singular data point.\n","\n","\n","[For a better understanding check out this video](https://www.youtube.com/watch?v=6zk6uQSuXqs)\n","\n","[`GridSearchCV` Class's Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)"]},{"cell_type":"markdown","metadata":{"id":"_e8pPYigmH-_","colab_type":"text"},"source":["<br>\n","\n","### Make Pipelines, One For Each Model Algorithm\n","\n","We will start by tranforming the `features` matrix data using the `StandardScalar` object's `fit_transform()` method, which will normalize all of the data. We will then pass the transformed `features` matrix into the estimator's `fit` method along with the unmodified `labels` series as parameters."]},{"cell_type":"markdown","metadata":{"id":"t4WrwfFgnw5b","colab_type":"text"},"source":["#### Import The Following Libraries From Sklearn's Library\n","- `make_pipeline` function, which will help us create a pipeline object\n","- `StandardScalar` class, which will normalize the dataset\n","- `LinearRegression` estimator class \n","- `Lasso` estimator class\n","- `Ridge` estimator class\n","- `ElasticNet` estimator class \n","\n","**Remember:** You can tell the difference between a class and a function by the case sensivity\n","\n","- A **class** will be captialized\n","- A **function** will be lowercase"]},{"cell_type":"code","metadata":{"id":"YtuK_fn4jdLC","colab_type":"code","colab":{}},"source":["# Helper for pipelines\n","from sklearn.pipeline import make_pipeline\n","\n","# Models/Estimators\n","from sklearn.linear_model import LinearRegression\n","from sklearn.linear_model import Lasso\n","from sklearn.linear_model import Ridge\n","from sklearn.linear_model import ElasticNet\n","\n","# Helper for normailizing dataset\n","from sklearn.preprocessing import StandardScaler"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x_LVf7bC_Z7t","colab_type":"text"},"source":["Resources:\n","- [Linear Regression Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n","- [Lasso Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso)\n","- [Ridge Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge)\n","- [ElasticNet Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html#sklearn.linear_model.ElasticNet)"]},{"cell_type":"markdown","metadata":{"id":"7wNrbht_oAhA","colab_type":"text"},"source":["### Create Pipeline Dictionary\n","Create a dictionary called `pipelines_dict` to hold multiple pipeline objects. The `key` of the dictionary should be the model's name and the `value` should be an instanciated pipeline object, which can be made by invoking the `make_pipeline` function. We will create a key and value pair for each estimator.\n","\n","<br>\n","\n","#### Understanding A Dictionary\n","A `dictionary` is an unordered collection of data values, used to store data values like a map, which unlike other Data Types that hold only single value as an element, a dictionary holds `key:value` pair. A dictionary has a `key`, and each key maps to a `unique value`. A dictionary is useful when you are trying locate a specific value based on a key in a collection, opposed to iterating over an array/list to get to find specific value. Picture that you have to cycle through a really long list of items just to find the one you were looking for. Is cycling through all those items really necessary? Technically speaking cycling through a list takes longer time and more computer performance, something we need to be mindful of when working with machine learning with big data. A dictionary allows us to quickly access a value based on a unique key, without having to iterate, or cycle, through all elements in this collection.\n","\n","It's a good time to mention that values of dictionaries can be dictionaries themselves. \n","\n","[Dictionary reference](https://www.geeksforgeeks.org/python-dictionary/)\n","\n","<br> \n","\n","#### Use The `make_pipeline()` Function To Set `Values` Of The Dictionary:\n","- The `keys` should be the estimator's name. \n","- Pass in an instanciated `StandardScalar` object as the first argument to the `make_pipeline` function\n","- Pass in an instanciated estimator object as the second argument to the `make_pipline` function\n","\n","**Note:** Both the `StandardScalar` and `estimator objects will be instanciated with no parameters being passed into their constructor function\n","\n","**Note:** We will make multiple dictionaries so it is important to keep using the same `key`, so that later we can use a `key` to access values from multiple dicitonaries."]},{"cell_type":"code","metadata":{"id":"spBHCI_0nuvc","colab_type":"code","colab":{}},"source":["pipelines_dict = {\n","    \"linear_regression\": make_pipeline(StandardScaler(), LinearRegression()),\n","    \"lasso\": make_pipeline(StandardScaler(), Lasso()),\n","    \"ridge\": make_pipeline(StandardScaler(), Ridge()),\n","    \"elastic_net\": make_pipeline(StandardScaler(), ElasticNet())\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LrSUvCbg-3VJ","colab_type":"text"},"source":["<br>\n","\n","### Make Multiple Hyper-Parameter Dictionaries\n","One for each machine learning estimator algorithm.\n","\n","**Hint:**\n","You can find out what hyper-parameters an estimator has by using the pipeline object's `get_params` method. This will return a dictionary. \n","\n","\n","<br>\n","\n","#### To view the pipeline dictionary, print the dictionary using a pipeline object's `get_params` method: \n","```python\n","# linear regression pipeline's paramater dicitonary\n","pipelines_dict['linear_regression'].get_params()\n","```\n","**Note:** We will access a pipeline object through using bracket notation to access a value from the `pipelines_dict` dictionary. The key will go inside the brackets to return the desired value (pipeline object).\n","\n","[Accessing Dictionary Values Reference](https://realpython.com/python-dicts/#accessing-dictionary-values)\n"]},{"cell_type":"markdown","metadata":{"id":"Mtu8fBjn3SrQ","colab_type":"text"},"source":["#### Print The Linear Regression Pipeline Parameters Using The Pipeline's `get_params` method"]},{"cell_type":"code","metadata":{"id":"x-u3-X1S6HKp","colab_type":"code","outputId":"298ffa56-1207-410a-e257-f2b841c37b38","executionInfo":{"status":"ok","timestamp":1576878622402,"user_tz":480,"elapsed":1925,"user":{"displayName":"Matt Hess","photoUrl":"","userId":"00800340733434721825"}},"colab":{"base_uri":"https://localhost:8080/","height":261}},"source":["# linear regression pipeline's paramater dicitonary\n","pipelines_dict['linear_regression'].get_params()"],"execution_count":138,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'linearregression': LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False),\n"," 'linearregression__copy_X': True,\n"," 'linearregression__fit_intercept': True,\n"," 'linearregression__n_jobs': None,\n"," 'linearregression__normalize': False,\n"," 'memory': None,\n"," 'standardscaler': StandardScaler(copy=True, with_mean=True, with_std=True),\n"," 'standardscaler__copy': True,\n"," 'standardscaler__with_mean': True,\n"," 'standardscaler__with_std': True,\n"," 'steps': [('standardscaler',\n","   StandardScaler(copy=True, with_mean=True, with_std=True)),\n","  ('linearregression',\n","   LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False))],\n"," 'verbose': False}"]},"metadata":{"tags":[]},"execution_count":138}]},{"cell_type":"markdown","metadata":{"id":"xYHVtIXE6Gun","colab_type":"text"},"source":["\n","#### Invoke the `keys` method on the linear regression pipeline's parameter dictionary to view all of the pipeline's parameter names\n","\n","```python\n","# linear regression pipeline's parameter dicitonary keys\n","pipelines_dict['linear_regression'].get_params().keys()\n","```"]},{"cell_type":"code","metadata":{"id":"WJKIoj6H_XcM","colab_type":"code","outputId":"5a6793a0-59d0-4225-ce24-e8c806f78e35","executionInfo":{"status":"ok","timestamp":1576878622403,"user_tz":480,"elapsed":1895,"user":{"displayName":"Matt Hess","photoUrl":"","userId":"00800340733434721825"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["pipelines_dict['linear_regression'].get_params().keys()"],"execution_count":139,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['memory', 'steps', 'verbose', 'standardscaler', 'linearregression', 'standardscaler__copy', 'standardscaler__with_mean', 'standardscaler__with_std', 'linearregression__copy_X', 'linearregression__fit_intercept', 'linearregression__n_jobs', 'linearregression__normalize'])"]},"metadata":{"tags":[]},"execution_count":139}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"P3h9l_mm7GX0"},"source":["#### Invoke the `values()` method on the linear regression pipeline's parameter dictionary to view all the  pipeline's parameter values\n","```python\n","# pipeline dicitonary values\n","pipelines_dict['linear_regression'].get_params().values()\n","```"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"CJQ72exM7GX4","outputId":"90b04c02-c106-4a0a-9827-df7916f8a5cf","executionInfo":{"status":"ok","timestamp":1576878622403,"user_tz":480,"elapsed":1865,"user":{"displayName":"Matt Hess","photoUrl":"","userId":"00800340733434721825"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["pipelines_dict['linear_regression'].get_params().values()"],"execution_count":140,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_values([None, [('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('linearregression', LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False))], False, StandardScaler(copy=True, with_mean=True, with_std=True), LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False), True, True, True, True, True, None, False])"]},"metadata":{"tags":[]},"execution_count":140}]},{"cell_type":"markdown","metadata":{"id":"5GTMQFo__YzL","colab_type":"text"},"source":["### Create Multiple Dictionaries To Hold The Different Hyper-Parameters For Each Indvidual Estimator\n","\n","For each dictionary:\n","- Name the dictionary with respect to the model name\n","- The `key` will be a unique estimator hyper-parameter name\n","- The `value` will be an array, filled with multiple unique values for that specific hyper-parameter\n","\n","#### <span style=\"color:red\"> Important Note: </span>\n","When creating an estimator's hyper-parameter dictionary we need to make sure we are using the pipeline object's dictionary `keys`, not the actual estimator object's dictionary `keys` **i.e.** `LinearRegression()`. \n"," \n","##### Don't Do:\n","\n","<del>\n","\n","```python \n","# Do not use the actual estimator object's keys\n","LinearRegression().get_params().keys()\n","\n","linear_regression_hyper_parameter_dict = {\n","  'fit_intercept': [True, False] # When we fit, an error\n","}\n","```\n","\n","</del>\n","\n","When we perform fitting the model with the following hyper-parameters we will get the following error if we do not use the proper hyper-parameter names:\n","\n","```ValueError: Invalid parameter fit_intercept for estimator Pipeline```\n","\n","This is because we need to use the pipeline's hyper-parameters naming convention instead: \n","\n","```python \n","# Get Pipeline's hyper-parameter options instead due to naming conventions sklearn follows\n","pipelines_dict['linear_regression'].get_params().keys()\n","\n","# Use Pipeline's hyper-parameter options instead due to the naming conventions sklearn follows\n","linear_regression_hyper_parameter_dict = {\n","    'linearregression__fit_intercept': [True, False],\n","}\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"Z3IyRXHz8bOE","colab_type":"text"},"source":["#### Create Linear Regression Hyper-Parameter Dictionary"]},{"cell_type":"code","metadata":{"id":"2Vzz3WUDnDzs","colab_type":"code","outputId":"23454e96-2df5-4c9d-85b1-3bd151548ea6","executionInfo":{"status":"ok","timestamp":1576878622404,"user_tz":480,"elapsed":1844,"user":{"displayName":"Matt Hess","photoUrl":"","userId":"00800340733434721825"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["print(pipelines_dict['linear_regression'].get_params().keys())\n","\n","# Use Pipeline's hyper-parameter options instead due to the naming conventions sklearn follows\n","linear_regression_hyper_parameter_dict = {\n","    'linearregression__fit_intercept': [True, False],\n","}"],"execution_count":141,"outputs":[{"output_type":"stream","text":["dict_keys(['memory', 'steps', 'verbose', 'standardscaler', 'linearregression', 'standardscaler__copy', 'standardscaler__with_mean', 'standardscaler__with_std', 'linearregression__copy_X', 'linearregression__fit_intercept', 'linearregression__n_jobs', 'linearregression__normalize'])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hmuOSsaH8k1S","colab_type":"text"},"source":["#### Create Lasso Hyper-Parameter Dictionary"]},{"cell_type":"code","metadata":{"id":"Kd4fdS978vfW","colab_type":"code","outputId":"c4ee40ed-1dc6-48ff-8003-9e751cc7ffae","executionInfo":{"status":"ok","timestamp":1576878622404,"user_tz":480,"elapsed":1811,"user":{"displayName":"Matt Hess","photoUrl":"","userId":"00800340733434721825"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["print(pipelines_dict['lasso'].get_params().keys())\n","\n","# Use Pipeline's hyper-parameter options instead due to the naming conventions sklearn follows\n","lasso_hyper_parameter_dict = {\n","    'lasso__random_state': [1],\n","    'lasso__fit_intercept': [True, False],\n","    'lasso__alpha': [0.01, 0.1, 0.5, 0.7, 1, 2, 5, 10],\n","}"],"execution_count":142,"outputs":[{"output_type":"stream","text":["dict_keys(['memory', 'steps', 'verbose', 'standardscaler', 'lasso', 'standardscaler__copy', 'standardscaler__with_mean', 'standardscaler__with_std', 'lasso__alpha', 'lasso__copy_X', 'lasso__fit_intercept', 'lasso__max_iter', 'lasso__normalize', 'lasso__positive', 'lasso__precompute', 'lasso__random_state', 'lasso__selection', 'lasso__tol', 'lasso__warm_start'])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3QGsQsRI_6_S","colab_type":"text"},"source":["#### Create Ridge Hyper-Parameter Dictionary"]},{"cell_type":"code","metadata":{"id":"eAjFOdNrAZlF","colab_type":"code","outputId":"b34b7964-7e44-42ab-9d5e-83600e4cf4d5","executionInfo":{"status":"ok","timestamp":1576878622405,"user_tz":480,"elapsed":1736,"user":{"displayName":"Matt Hess","photoUrl":"","userId":"00800340733434721825"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["print(pipelines_dict['ridge'].get_params().keys())\n","\n","# Use Pipeline's hyper-parameter options instead due to the naming conventions sklearn follows\n","ridge_hyper_parameter_dict = {\n","    'ridge__random_state': [1],\n","    'ridge__fit_intercept': [True, False],\n","    'ridge__alpha': [0.01, 0.1, 0.5, 0.7, 1, 2, 5, 10],\n","}"],"execution_count":143,"outputs":[{"output_type":"stream","text":["dict_keys(['memory', 'steps', 'verbose', 'standardscaler', 'ridge', 'standardscaler__copy', 'standardscaler__with_mean', 'standardscaler__with_std', 'ridge__alpha', 'ridge__copy_X', 'ridge__fit_intercept', 'ridge__max_iter', 'ridge__normalize', 'ridge__random_state', 'ridge__solver', 'ridge__tol'])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oply4gIND5Zu","colab_type":"text"},"source":["#### Create ElasticNet Hyper-Parameter Dictionary"]},{"cell_type":"code","metadata":{"id":"i7-gRFFJEBtN","colab_type":"code","outputId":"816816f6-c775-49ef-a9c6-f3141189f7dd","executionInfo":{"status":"ok","timestamp":1576878622406,"user_tz":480,"elapsed":1709,"user":{"displayName":"Matt Hess","photoUrl":"","userId":"00800340733434721825"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["print(pipelines_dict['elastic_net'].get_params().keys())\n","\n","# Use Pipeline's hyper-parameter options instead due to the naming conventions sklearn follows\n","elastic_net_hyper_parameter_dict = {\n","    'elasticnet__random_state': [1],\n","    'elasticnet__fit_intercept': [True, False],\n","    'elasticnet__alpha': [0.01, 0.1, 0.5, 0.7, 1, 2, 5, 10],\n","    'elasticnet__l1_ratio': [0.01, 0.1, 0.5, 0.7, 0.8, 0.9, 1]\n","}"],"execution_count":144,"outputs":[{"output_type":"stream","text":["dict_keys(['memory', 'steps', 'verbose', 'standardscaler', 'elasticnet', 'standardscaler__copy', 'standardscaler__with_mean', 'standardscaler__with_std', 'elasticnet__alpha', 'elasticnet__copy_X', 'elasticnet__fit_intercept', 'elasticnet__l1_ratio', 'elasticnet__max_iter', 'elasticnet__normalize', 'elasticnet__positive', 'elasticnet__precompute', 'elasticnet__random_state', 'elasticnet__selection', 'elasticnet__tol', 'elasticnet__warm_start'])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oAIO8ieDJIkw","colab_type":"text"},"source":["### Create a Dictionary To Group All Individual Model Hyper-Parameter Dictionaries \n","Call this dictionary `hyper_parameters_dict`.\n","Each `key` should match the same `key` name used for the `pipelines_dict`, and each value should be assigned that model's hype-parameter dictionary. \n","\n","**Note:** It is important to use the same `key` name so that later when we perform the cross valudation grid search we can loop through the `key` names once and use this key to access multple values from different dictionaries with similar key names but different values.\n","\n","For Example:\n","```python\n","hyper_parameters_dict = {\n","    \"linear_regression\": linear_regression_hyper_parameter_dict,\n","    ...\n","}\n","```\n"]},{"cell_type":"code","metadata":{"id":"vRtmipHpJfBx","colab_type":"code","colab":{}},"source":["hyper_parameters_dict = {\n","    \"linear_regression\": linear_regression_hyper_parameter_dict,\n","    \"lasso\": lasso_hyper_parameter_dict,\n","    \"ridge\": ridge_hyper_parameter_dict,\n","    \"elastic_net\": elastic_net_hyper_parameter_dict\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4pAMAeJAFIWx","colab_type":"text"},"source":["<hr>\n","\n","### Perform A Cross Validation Grid Search With Each Model\n","Now that we have a `pipelines_dict` and `hyper_parameter_dict` dictionary we can perform multiple cross validation grid searches, one for each model. \n","\n","We will do this by looping through a dictionary's keys to get access to the `key` names, we will then use the `key` names to access values from mutliple dictionaries.\n","\n","We will loop through the key names using the dictionary's `keys` method:\n","\n","```python \n","# Loop through model names\n","for model_name in pipelines_dict.keys():\n","  print(model_name)\n","```\n","\n","\n","[Looping Through Dictionary Keys Reference](https://realpython.com/iterate-through-dictionary-python/#iterating-through-keys)"]},{"cell_type":"markdown","metadata":{"id":"3XnUAQoyNJS2","colab_type":"text"},"source":["#### Print Key Names By Looping Through The `pipelines_dict` Keys\n","This exercise will show you that we can access `key` names from dictionaries. We will use these `key` names to perform a cross validation grid search for each model to find the highest performing model with specific hyper-parameters set. "]},{"cell_type":"code","metadata":{"id":"A770JK3cNWPu","colab_type":"code","outputId":"eba1f5ca-2786-44fe-fa28-90ba35039f47","executionInfo":{"status":"ok","timestamp":1576878622408,"user_tz":480,"elapsed":1667,"user":{"displayName":"Matt Hess","photoUrl":"","userId":"00800340733434721825"}},"colab":{"base_uri":"https://localhost:8080/","height":82}},"source":["# Loop through model names\n","for model_name in pipelines_dict.keys():\n","  print(model_name)"],"execution_count":146,"outputs":[{"output_type":"stream","text":["linear_regression\n","lasso\n","ridge\n","elastic_net\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Izt-7MFfZUSf"},"source":["<br> \n","\n","\n","### Import `GridSearchCv` Class From Sklearn's Library"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Si2AzPC_ZUSl","colab":{}},"source":["# Helper for cross-validation\n","from sklearn.model_selection import GridSearchCV"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8XR2R8JtOlGQ","colab_type":"text"},"source":["### Create A Dictionary To Hold All Of The Highest Performing Models\n","Create an empty dictionary called, `best_performing_models_dict`. Once we have created this dictionary, we will then create a loop to populate this new dictionary. This dictionary will contain an entry for each model, each entry will contain a model with a specific hyper-parameter set which maximizes the performance for each specific model. "]},{"cell_type":"code","metadata":{"id":"xmS3KBjEQSLE","colab_type":"code","colab":{}},"source":["best_performing_models_dict = {}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nFRTy5yAk9A4","colab_type":"text"},"source":["### Create A Loop To Perform A Cross Validation Grid Search For Each Model To Populate the `best_performing_models_dict`\n","\n","\n","#### When looping, for each iteration of the loop, set the following parameters of the `GridSearchCV` object initialization:\n","- `estimator` parameter will be assigned with it's corresponding `pipeline` object instance as its argument\n","- `param_grid` parameter will be assigned with it's corresponding  `hyper-parameter-dictionary` instance\n","- `return_train_score` parameter will be assigned with the value `True`\n","- `refit` parameter will be assigned with the value `True`\n","- `n_jobs` parameter will be assigned the value `-1` to use all available cpu power\n","\n","**Note:** We will index the `pipelines_dict` and `hyper_parameters_dict` with the current iterations model name and create key value pairs for the `best_performing_models_dict` using that same model name as the key and assigning the value to that model's `best_estimator_` attribute\n","\n","[`GridSearchCV` Class's Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"KmAY8ghU9ivt","colab":{}},"source":["pipelines_dict = {\n","    \"linear_regression\": make_pipeline(StandardScaler(), LinearRegression()),\n","    # \"lasso\": make_pipeline(StandardScaler(), Lasso()),\n","    # \"ridge\": make_pipeline(StandardScaler(), Ridge()),\n","    # \"elastic_net\": make_pipeline(StandardScaler(), ElasticNet())\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"b_8FwW0f9l59","colab":{}},"source":["hyper_parameters_dict = {\n","    \"linear_regression\": linear_regression_hyper_parameter_dict,\n","    # \"lasso\": lasso_hyper_parameter_dict,\n","    # \"ridge\": ridge_hyper_parameter_dict,\n","    # \"elastic_net\": elastic_net_hyper_parameter_dict\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YQjlCgNvSqaJ","colab_type":"code","outputId":"78991b20-3104-4efb-bd71-3dd6f7d9b65b","executionInfo":{"status":"ok","timestamp":1576878624591,"user_tz":480,"elapsed":3776,"user":{"displayName":"Matt Hess","photoUrl":"","userId":"00800340733434721825"}},"colab":{"base_uri":"https://localhost:8080/","height":135}},"source":["# Loop through model names\n","for model_name in pipelines_dict.keys():\n","\n","  # Print to screen which model is being fitted\n","  print(\"Searching for the best {} model:\".format(model_name))\n","\n","  # Preparation step for finding the best estimator with a specific hyper-parameters set.\n","  model = GridSearchCV(pipelines_dict[model_name], hyper_parameters_dict[model_name], cv=10, n_jobs=-1, return_train_score=True, refit=True) \n","\n","  # Fit model to training data\n","  model.fit(X_train, y_train)\n","\n","  # Print the model's mean performing score \n","  print(\"{} model's cross validation mean performing score: {}\".format(model_name, model.best_score_))\n","\n","  # Populate the best_performing_models_dict with the model's best_estimator_ attribute\n","  best_performing_models_dict[model_name] = model.best_estimator_\n","  \n","  # Print if a specific model had been stored   \n","  if best_performing_models_dict[model_name] != None:\n","    print(\"Best {} model stored \\n\".format(model_name))"],"execution_count":151,"outputs":[{"output_type":"stream","text":["Searching for the best linear_regression model:\n","linear_regression model's cross validation mean performing score: 0.404873450982989\n","Best linear_regression model stored \n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n","  DeprecationWarning)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"iF5743tsPBSn","colab_type":"text"},"source":["<br>\n","\n","#### **Note:** The data type of the model instance is of type GridSearchCv\n"]},{"cell_type":"markdown","metadata":{"id":"xUHOG3XPvvoL","colab_type":"text"},"source":["### Print the Cross Validation Results For the Last Model Run In The Cross Validation Search\n","Use the `cv_results_` property on the current model instance.\n","\n","```python\n","# cross validation results\n","model.cv_results_\n","```\n","\n","**Note:** the current model instance is the last model that was run in the cross validation grid search. If you wanted to see the results for each model you would print the `cv_results` inside the loop above."]},{"cell_type":"code","metadata":{"id":"IufCA7qS0y1Q","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":586},"outputId":"6ee06cf0-7e4f-4cb7-9ce6-6735ef4d9343","executionInfo":{"status":"ok","timestamp":1576878624593,"user_tz":480,"elapsed":3738,"user":{"displayName":"Matt Hess","photoUrl":"","userId":"00800340733434721825"}}},"source":["# cross validation results\n","display(model.cv_results_)"],"execution_count":152,"outputs":[{"output_type":"display_data","data":{"text/plain":["{'mean_fit_time': array([0.01072845, 0.01089587]),\n"," 'mean_score_time': array([0.00249472, 0.00247881]),\n"," 'mean_test_score': array([ 0.40487345, -7.53932783]),\n"," 'mean_train_score': array([ 0.45632874, -7.47182603]),\n"," 'param_linearregression__fit_intercept': masked_array(data=[True, False],\n","              mask=[False, False],\n","        fill_value='?',\n","             dtype=object),\n"," 'params': [{'linearregression__fit_intercept': True},\n","  {'linearregression__fit_intercept': False}],\n"," 'rank_test_score': array([1, 2], dtype=int32),\n"," 'split0_test_score': array([ 0.38213891, -7.42620115]),\n"," 'split0_train_score': array([ 0.45946899, -7.44700949]),\n"," 'split1_test_score': array([ 0.37662457, -8.35941026]),\n"," 'split1_train_score': array([ 0.45942683, -7.44587178]),\n"," 'split2_test_score': array([ 0.47636875, -8.50996462]),\n"," 'split2_train_score': array([ 0.45107483, -7.4396096 ]),\n"," 'split3_test_score': array([ 0.38005467, -7.83495383]),\n"," 'split3_train_score': array([ 0.4598655 , -7.35421953]),\n"," 'split4_test_score': array([ 0.38564597, -7.17293593]),\n"," 'split4_train_score': array([ 0.45987829, -7.51078231]),\n"," 'split5_test_score': array([ 0.44074452, -7.34545859]),\n"," 'split5_train_score': array([ 0.45402782, -7.52446358]),\n"," 'split6_test_score': array([ 0.26995459, -7.19116983]),\n"," 'split6_train_score': array([ 0.45934298, -7.51501883]),\n"," 'split7_test_score': array([ 0.45363618, -7.039831  ]),\n"," 'split7_train_score': array([ 0.45268984, -7.52411127]),\n"," 'split8_test_score': array([ 0.52097221, -7.20580676]),\n"," 'split8_train_score': array([ 0.44462166, -7.47262887]),\n"," 'split9_test_score': array([ 0.36275106, -7.29683414]),\n"," 'split9_train_score': array([ 0.46289063, -7.48454503]),\n"," 'std_fit_time': array([0.00167393, 0.00222609]),\n"," 'std_score_time': array([0.00021774, 0.00038196]),\n"," 'std_test_score': array([0.06665903, 0.49311906]),\n"," 'std_train_score': array([0.00529062, 0.05020832])}"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"CcuWmrlq7bzx"},"source":["<hr>\n","\n","## Evaluate Highest Performing Models Score Using Training Data\n","We will now find out how well each of the highest performing models with a specific hyper-parameter value set performs.\n","\n","Use the `GridSearchCV` class's `score` method using the training data. \n","\n","**Note:** The reason why we can train and evaluate with the same data is because we performed cross validation. \n","\n","This `score` method will use the best estimator's scoring function, each estimator might have a different scoring function.\n","\n","<br>\n","\n","**Steps:**\n","1. Create an array called `highest_performing_models` to store the best models incase there are multiple models with the same scores\n","2. Loop through model names\n","3. Store the current iteration's model score using training data of the best estimator using the training data in a temporary variable \n","4. Print the model name and score using the temporary variable holding the current iteration's model\n","5. Store the highest performing model or models, if there are multiple models with the same highest score, name and score inside of a tuple and then store that tuple inside the `highest_performing_models` array.   \n","\n","**Note:** When storing the highest performing model or models make sure to handle the following cases:\n","- Handling of first highest performing model using training data \n","- Handling if we find a model with a new high training score\n","- Handling of multiple models with the same highest training  score\n","\n","Refrences:\n","- [`GridSearchCV` class's `score` method documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV.score)\n","- [Tuples Reference](https://www.tutorialspoint.com/python/python_tuples.htm)\n"]},{"cell_type":"code","metadata":{"id":"evK-KYGo7l63","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":33},"outputId":"1e01d466-177e-4c8c-991a-3356ea99f41c","executionInfo":{"status":"ok","timestamp":1576878624594,"user_tz":480,"elapsed":3716,"user":{"displayName":"Matt Hess","photoUrl":"","userId":"00800340733434721825"}}},"source":["# Create an array to store the best models incase there are multiple models with the same scores\n","highest_performing_models = []\n","\n","# Loop through model names\n","for model_name in pipelines_dict.keys():\n","  # Store the score of the best estimator using the training data\n","  training_model_score = best_performing_models_dict[model_name].score(X_train, y_train)\n","\n","  # Print the model name and score\n","  print(\"{} training model score: {}\".format(model_name, training_model_score))\n","\n","  ## Store Highest performing model or models if there are multiple models with the same highest score \n","  # Handling of first highest performing model \n","  if len(highest_performing_models) == 0:\n","    value = (model_name, training_model_score)\n","    highest_performing_models.append(value)\n","  \n","  # Handling if we find a model with a new high score\n","  elif training_model_score > highest_performing_models[0][1]:\n","    # clear array if we have found a new high score to wype the old records\n","    highest_performing_models = []\n","    value = (model_name, training_model_score)\n","    highest_performing_models = [value]\n","  \n","  # Handling of multiple models with the same highest score\n","  elif training_model_score == highest_performing_models[0][1]:\n","    value = (model_name, training_model_score)\n","    highest_performing_models.append(value)"],"execution_count":153,"outputs":[{"output_type":"stream","text":["linear_regression training model score: 0.4541561568926857\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mjjqJgpFz4HI","colab_type":"text"},"source":["### Print The Top Performing Model Name(s), Score, and Hyper-Parameter Set(s) Built \n","**Note:** Handle the following cases:\n","- A single model having the highest training score\n","- Multiple models have the same high training  score\n","\n","\n","\n","**Remember:** Each `value` of the `best_performing_models_dict` is actually a `Pipeline` object. Each `pipeline` object has a `score` method, use this to evaluate the model on the training set. \n","\n","<span style=\"color:red\">**It's really important to always understand the type of object you are working with, use that object's documentation to guide you in the right direction**</span>\n","\n","**Hint:**\n","Use the `highest_performing_models` array to access the name(s) of the highest performing model(s). Remember each item in the array is a tuple of two values, the `model_name` and the `training_model_score`. Once you have the access to the `model_name` use that to access the `best_performing_models_dict` using the `model_name` and invoke the `get_params` method to find out the hyper-parameter set.\n","\n","[Indexing tuples Reference](https://www.tutorialspoint.com/python/python_tuples.htm)"]},{"cell_type":"code","metadata":{"id":"m-yu_uUPyVhO","colab_type":"code","outputId":"03befb59-b334-4119-a0ef-8d6fe6839607","executionInfo":{"status":"ok","timestamp":1576878624595,"user_tz":480,"elapsed":3689,"user":{"displayName":"Matt Hess","photoUrl":"","userId":"00800340733434721825"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["# Handling of a single model having the highest score\n","if len(highest_performing_models) == 1:\n","  for model_name, training_model_score in highest_performing_models:\n","    print(\"{} is the highest performing model, with a training score of {}\".format(model_name, training_model_score))\n","    print(\"{}'s highest performing model hyper-parameter set: \\n {} \\n\".format(model_name, best_performing_models_dict[model_name].get_params()))\n","\n","# Handling of multiple models have the same high score\n","else:\n","  print(\"Top performing models with a training score of {}: \\n\".format(highest_performing_models[0][1]))\n","  for model_name, training_model_score in highest_performing_models:\n","    print(\"{} model hyper-parameter set:\".format(model_name))\n","    print(\"{}\\n\".format(best_performing_models_dict[model_name].get_params()))"],"execution_count":154,"outputs":[{"output_type":"stream","text":["linear_regression is the highest performing model, with a training score of 0.4541561568926857\n","linear_regression's highest performing model hyper-parameter set: \n"," {'memory': None, 'steps': [('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('linearregression', LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False))], 'verbose': False, 'standardscaler': StandardScaler(copy=True, with_mean=True, with_std=True), 'linearregression': LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False), 'standardscaler__copy': True, 'standardscaler__with_mean': True, 'standardscaler__with_std': True, 'linearregression__copy_X': True, 'linearregression__fit_intercept': True, 'linearregression__n_jobs': None, 'linearregression__normalize': False} \n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"iGcw-h6XDfX7"},"source":["<br>\n","\n","### Question\n","What can we conclude from looking at the all the best_estimator's scores?\n","\n","### Answer:\n","The most performant model is the lasso algorithm\n","\n","<br>\n","\n","#### **Important note:** \n","The `best_estimator`'s hyper-parameter values were specifically chosen because of the data it was trained on. If you manipulate the data and then re-fit the model we might notice different values.  "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"WkapUmD0Etsn"},"source":["<hr>\n","\n","<br>\n","\n","## Evaluate The Best Performing Model(s) Using The Testing Data\n","\n","**Remember:** Each `value` of the `best_performing_models_dict` is actually a `Pipeline` object. Each `pipeline` object has a `score` method, use this to evaluate the model on the testing set. \n","\n","<span style=\"color:red\">**It's really important to always understand the type of object you are working with, use that object's documentation to guide you in the right direction**</span>\n","\n","**Hint:**\n","Use the `highest_performing_models` array to access the name(s) of the highest performing model(s). Remember each item in the array is a tuple of two values, the `model_name` and the `model_score`. Once you have the access to the `model_name` use that to access the `pipeline` object(s) from the `best_performing_models_dict`. Once you have access to the `pipeline` object(s) then use the `score` method passing in `X_test` and `y_test` as arguments.  \n","\n","[Indexing tuples Reference](https://www.tutorialspoint.com/python/python_tuples.htm)"]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"6a6574dc-c943-47ca-c3da-e3708cc5052b","executionInfo":{"status":"ok","timestamp":1576878624596,"user_tz":480,"elapsed":3670,"user":{"displayName":"Matt Hess","photoUrl":"","userId":"00800340733434721825"}},"id":"V5DKobc2fyPr","colab":{"base_uri":"https://localhost:8080/","height":66}},"source":["# Handling of a single model having the highest score\n","if len(highest_performing_models) == 1:\n","  for model_name, training_model_score in highest_performing_models:\n","    print('Model name: {}'.format(model_name))\n","    print(\"Training score: {}\".format(training_model_score))\n","    print(\"Testing score: {}\".format(best_performing_models_dict[model_name].score(X_test, y_test)))\n","\n","# Handling of multiple models have the same high score\n","else:\n","  print(\"Top performing models with a training score of {}: \\n\".format(highest_performing_models[0][1]))\n","  for model_name, training_model_score in highest_performing_models:\n","    print('Model name: {}'.format(model_name))\n","    print(\"Testing score: {}\".format(best_performing_models_dict[model_name].score(X_test, y_test)))"],"execution_count":155,"outputs":[{"output_type":"stream","text":["Model name: linear_regression\n","Training score: 0.4541561568926857\n","Testing score: 0.46674832991442416\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ztkuhwGwslI7","colab_type":"text"},"source":["<hr>\n","\n","# Make Predictions Using Highest Performing Model(s) and Testing Data\n","Now that we have the highest scoring fitted model(s) we can now make a prediction using the testing set.\n","\n","Make predictions using the first ten observations of the testing data to get a feel of how well the highest performing model performed. We will be interested in the residual, or the difference between the true value and the predicted value. \n","\n","\n","If we have more than one model with the same highest performing score then make multiple series of predictions. Prepend the model name to the predictions series. \n","For example:\n","```python\n","# Create predictions with testing data\n","lasso_test_predictions = pd.Series(best_performing_models_dict['lasso'].predict(X_test))\n","# Re-assign indexes\n","lasso_test_predictions.index = X_test.index\n","\n","print(\"Lasso predictions:\")\n","display(lasso_test_predictions.head(10))\n","print(\"True Values:\")\n","display(y_test.head(10))\n","\n","\n","# Create predictions with testing data\n","ridge_test_predictions = pd.Series(best_performing_models_dict['ridge'].predict(X_test))\n","# Re-assign indexes\n","ridge_test_predictions.index = X_test.index\n","\n","print(\"Ridge predictions:\")\n","display(ridge_test_predictions.head(10))\n","print(\"True Values:\")\n","display(y_test.head(10))\n","```\n","\n","**Note:**\n","After re-assigning the predictions series, each index of the  predictions series pertains to the same observation as the true values series."]},{"cell_type":"markdown","metadata":{"id":"KFrYxuPKPTfJ","colab_type":"text"},"source":["### Display the predictions using the testing set"]},{"cell_type":"code","metadata":{"id":"lH4jyt6Ft_hu","colab_type":"code","outputId":"b2e3d1fc-7bd4-4a54-a1f8-bb4ba1eaba19","executionInfo":{"status":"ok","timestamp":1576884259556,"user_tz":480,"elapsed":918,"user":{"displayName":"Matt Hess","photoUrl":"","userId":"00800340733434721825"}},"colab":{"base_uri":"https://localhost:8080/","height":212}},"source":["# Create predictions with testing data\n","lasso_test_predictions = pd.Series(best_performing_models_dict['linear_regression'].predict(X_test))\n","# Re-assign indexes\n","lasso_test_predictions.index = X_test.index\n"," \n","print(\"Lasso predictions:\")\n","display(lasso_test_predictions.head(10))"],"execution_count":205,"outputs":[{"output_type":"stream","text":["Lasso predictions:\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/plain":["1347    426755.002081\n","1868    656771.002081\n","887     493411.002081\n","650     584163.002081\n","102     386643.002081\n","1219    335059.002081\n","1165    357187.002081\n","282     463235.002081\n","1813    545915.002081\n","510     360867.002081\n","dtype: float64"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"5eG-iHrGuUu2","colab_type":"text"},"source":["### Compare Predictions To The True Values"]},{"cell_type":"code","metadata":{"id":"0GleCOR5uEJj","colab_type":"code","outputId":"28bf84ae-6e1a-4b36-da49-28b406d2b2c4","executionInfo":{"status":"ok","timestamp":1576878624597,"user_tz":480,"elapsed":3623,"user":{"displayName":"Matt Hess","photoUrl":"","userId":"00800340733434721825"}},"colab":{"base_uri":"https://localhost:8080/","height":212}},"source":["print(\"True Values:\")\n","display(y_test.head(10))"],"execution_count":157,"outputs":[{"output_type":"stream","text":["True Values:\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/plain":["1347    360000.0\n","1868    765000.0\n","887     333250.0\n","650     590000.0\n","102     369900.0\n","1219    429000.0\n","1165    384350.0\n","282     526275.0\n","1813    508000.0\n","510     306000.0\n","Name: tx_price, dtype: float64"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"9su0AV1nu_Bl","colab_type":"text"},"source":["<hr>\n","\n","<br>\n","\n","### What else can you do to boost the model performance?"]},{"cell_type":"markdown","metadata":{"id":"yWoX5aWwvaUM","colab_type":"text"},"source":["Iterate:\n","- Exploratory analysis\n","- Data cleaning \n","- Feature engineering\n","- Get More Data\n","- Create a loop to try building models with all combinations of features to find out which features are the most important to accurately predicting the target value "]}]}
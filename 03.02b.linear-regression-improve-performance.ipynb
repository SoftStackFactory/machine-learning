{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"03.02b.linear-regression-improve-performance.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"2mnArdogxOjj","colab_type":"text"},"source":["# Increase Linear Regression Performance \n","\n","We will learn how to improve the performance of a linear regression model. We will use this model to predict the transaction price of a house using a real estate dataset. \n","\n","## Overview: \n","1. Import the dataset\n","2. Separate the dataset's features from target variable\n","3. Split data into training and testing sets\n","4. Make a pipeline \n","5. Make a hyper-parameter dictionary\n","6. Perform a cross validaton grid search using the training set, and setting the following parameters of the `GridSearchCV` object initialization:\n","\n","  - `estimator` parameter will be assigned with the `pipeline` object instance as its argument\n","  - `param_grid` parameter will be assigned with the `hyper-parameter-dictionary` instance\n","\n","7. Identify best performing model using the training set with specific hyper-parameters set\n","8. Evaluate the best performing model using the testing data\n","\n","<hr>\n","\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"UAUVYQ7uUaDz","colab_type":"text"},"source":["## Import Required Libraries\n","\n","**Note:** You can tell the difference between a class and a function by the case sensivity. \n","\n","- A **class** will be captialized\n","- A **function** will be lowercase\n","- A **method**, or a function belonging to a class, will also be lowercase. You can call a method by invoking it through an instance of a class (instance method), or through a class definition (static method)\n","\n","References: \n","- [Understanding what a class is](https://www.hackerearth.com/practice/python/object-oriented-programming/classes-and-objects-i/tutorial/)\n","- [Differences between functions and methods](https://www.tutorialspoint.com/difference-between-method-and-function-in-python)\n","- [Different types of methods](https://www.bogotobogo.com/python/python_differences_between_static_method_and_class_method_instance_method.php)"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"EmHTYzZJxOjl","colab_type":"code","colab":{}},"source":["# Collection libraries \n","import numpy as np\n","import pandas as pd\n","\n","# Visual libraries \n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Helper for splitting training and testing data\n","from sklearn.model_selection import train_test_split\n","\n","# Model/Estimator\n","from sklearn.linear_model import LinearRegression\n","\n","# Helper for pipelines\n","from sklearn.pipeline import make_pipeline\n","\n","# Helper for normailizing dataset\n","from sklearn.preprocessing import StandardScaler\n","\n","# Helper for cross-validation\n","from sklearn.model_selection import GridSearchCV"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"cl119xCrxTjD"},"source":["#### Notes about imports with this notebook:\n","We will re-import some of the libraries when we use these modules, this is to get you used to importing and understanding their classes and functions. Reference the documentation to understand the libraries classes, methods, and functions. "]},{"cell_type":"markdown","metadata":{"id":"ny-A-wdQ7Yov","colab_type":"text"},"source":["## Load Data"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Es-ceD1D3ytV"},"source":["<hr>\n","\n","##### Mount Drive - **Google Colab Only Step**\n","\n","When using google colab in order to access files on our google drive we need to mount the drive by running the below python cell, then clicking the link it generates and pasting the code in the cell.\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"YKHWrhnJ3ytR","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"x5AzSk7g3ytO"},"source":["Change Directory To Access The Dependent Files - **Google Colab Only Step**"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"j9rgWUe03ytL","colab":{}},"source":["directory = \"student\"\n","if (directory == \"student\"):\n","  %cd drive/Colab\\ Notebooks/machine-learning/\n","else:\n","  %cd drive/Shared\\ drives/Rubrik/Data\\ Science\\ Track/machine-learning"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YmJlqpSrhYt0","colab_type":"text"},"source":["<hr> \n","<br>\n","\n","### Import Real Estate Dataset\n","Read in the real estate dataset using the path provided and store it in a variable called `df`.\n","\n","#### Import the cleaned real estate dataset\n","- Use pandas' `read_csv` function\n","\n","#### Pandas' `read_csv` parameters:\n","- `filepath_or_buffer` (string): path of csv to import\n","\n","```python \n","filepath_or_buffer = './data/cleaned_and_feature_engineered_real_estate.csv'\n","```"]},{"cell_type":"code","metadata":{"id":"eQvU4eCwh7GJ","colab_type":"code","colab":{}},"source":["df = pd.read_csv(filepath_or_buffer = './data/cleaned_and_feature_engineered_real_estate.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HZ1veSMBXdiZ","colab_type":"text"},"source":["### Show Head Of Datset"]},{"cell_type":"code","metadata":{"id":"xs4zgyBMXiCG","colab_type":"code","outputId":"cb05a2bd-8d78-4fa5-dd44-24c8534e9a41","executionInfo":{"status":"ok","timestamp":1574461711882,"user_tz":480,"elapsed":10237,"user":{"displayName":"Matt Hess","photoUrl":"","userId":"00800340733434721825"}},"colab":{"base_uri":"https://localhost:8080/","height":241}},"source":["df.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tx_price</th>\n","      <th>beds</th>\n","      <th>baths</th>\n","      <th>sqft</th>\n","      <th>year_built</th>\n","      <th>lot_size</th>\n","      <th>basement</th>\n","      <th>median_age</th>\n","      <th>married</th>\n","      <th>college_grad</th>\n","      <th>property_tax</th>\n","      <th>insurance</th>\n","      <th>median_school</th>\n","      <th>num_schools</th>\n","      <th>tx_year</th>\n","      <th>lifestyle_avg</th>\n","      <th>two_and_two</th>\n","      <th>exterior_walls_Brick</th>\n","      <th>exterior_walls_Brick veneer</th>\n","      <th>exterior_walls_Combination</th>\n","      <th>exterior_walls_Metal</th>\n","      <th>exterior_walls_Other</th>\n","      <th>exterior_walls_Siding (Alum/Vinyl)</th>\n","      <th>exterior_walls_Wood</th>\n","      <th>exterior_walls_missing</th>\n","      <th>roof_Asphalt</th>\n","      <th>roof_Composition Shingle</th>\n","      <th>roof_Other</th>\n","      <th>roof_Shake Shingle</th>\n","      <th>roof_missing</th>\n","      <th>property_type_Apartment / Condo / Townhouse</th>\n","      <th>property_type_Single-Family</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>295850.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>584.0</td>\n","      <td>2013.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>33.0</td>\n","      <td>65.0</td>\n","      <td>84.0</td>\n","      <td>234.0</td>\n","      <td>81.0</td>\n","      <td>9.0</td>\n","      <td>3.0</td>\n","      <td>2013.0</td>\n","      <td>1.493259</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>216500.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>612.0</td>\n","      <td>1965.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>39.0</td>\n","      <td>73.0</td>\n","      <td>69.0</td>\n","      <td>169.0</td>\n","      <td>51.0</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>2006.0</td>\n","      <td>0.676598</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>279900.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>615.0</td>\n","      <td>1963.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>28.0</td>\n","      <td>15.0</td>\n","      <td>86.0</td>\n","      <td>216.0</td>\n","      <td>74.0</td>\n","      <td>8.0</td>\n","      <td>3.0</td>\n","      <td>2012.0</td>\n","      <td>2.298254</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>379900.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>618.0</td>\n","      <td>2000.0</td>\n","      <td>33541.0</td>\n","      <td>0.0</td>\n","      <td>36.0</td>\n","      <td>25.0</td>\n","      <td>91.0</td>\n","      <td>265.0</td>\n","      <td>92.0</td>\n","      <td>9.0</td>\n","      <td>3.0</td>\n","      <td>2005.0</td>\n","      <td>2.473650</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>340000.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>634.0</td>\n","      <td>1992.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>37.0</td>\n","      <td>20.0</td>\n","      <td>75.0</td>\n","      <td>88.0</td>\n","      <td>30.0</td>\n","      <td>9.0</td>\n","      <td>3.0</td>\n","      <td>2002.0</td>\n","      <td>1.661371</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   tx_price  ...  property_type_Single-Family\n","0  295850.0  ...                            0\n","1  216500.0  ...                            0\n","2  279900.0  ...                            0\n","3  379900.0  ...                            0\n","4  340000.0  ...                            0\n","\n","[5 rows x 32 columns]"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"0BFt6HvMXlQH","colab_type":"text"},"source":["### Show Tail Of Dataset"]},{"cell_type":"code","metadata":{"id":"8lXDeQwAXpWU","colab_type":"code","outputId":"4981cccd-d49f-4c8a-bb1d-ab064444b44e","executionInfo":{"status":"ok","timestamp":1574461711893,"user_tz":480,"elapsed":10063,"user":{"displayName":"Matt Hess","photoUrl":"","userId":"00800340733434721825"}},"colab":{"base_uri":"https://localhost:8080/","height":241}},"source":["df.tail()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tx_price</th>\n","      <th>beds</th>\n","      <th>baths</th>\n","      <th>sqft</th>\n","      <th>year_built</th>\n","      <th>lot_size</th>\n","      <th>basement</th>\n","      <th>median_age</th>\n","      <th>married</th>\n","      <th>college_grad</th>\n","      <th>property_tax</th>\n","      <th>insurance</th>\n","      <th>median_school</th>\n","      <th>num_schools</th>\n","      <th>tx_year</th>\n","      <th>lifestyle_avg</th>\n","      <th>two_and_two</th>\n","      <th>exterior_walls_Brick</th>\n","      <th>exterior_walls_Brick veneer</th>\n","      <th>exterior_walls_Combination</th>\n","      <th>exterior_walls_Metal</th>\n","      <th>exterior_walls_Other</th>\n","      <th>exterior_walls_Siding (Alum/Vinyl)</th>\n","      <th>exterior_walls_Wood</th>\n","      <th>exterior_walls_missing</th>\n","      <th>roof_Asphalt</th>\n","      <th>roof_Composition Shingle</th>\n","      <th>roof_Other</th>\n","      <th>roof_Shake Shingle</th>\n","      <th>roof_missing</th>\n","      <th>property_type_Apartment / Condo / Townhouse</th>\n","      <th>property_type_Single-Family</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1877</th>\n","      <td>385000.0</td>\n","      <td>5.0</td>\n","      <td>6.0</td>\n","      <td>6381.0</td>\n","      <td>2004.0</td>\n","      <td>224334.0</td>\n","      <td>1.0</td>\n","      <td>46.0</td>\n","      <td>76.0</td>\n","      <td>87.0</td>\n","      <td>1250.0</td>\n","      <td>381.0</td>\n","      <td>10.0</td>\n","      <td>3.0</td>\n","      <td>2002.0</td>\n","      <td>-0.792553</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1878</th>\n","      <td>690000.0</td>\n","      <td>5.0</td>\n","      <td>6.0</td>\n","      <td>6501.0</td>\n","      <td>1956.0</td>\n","      <td>23086.0</td>\n","      <td>1.0</td>\n","      <td>42.0</td>\n","      <td>73.0</td>\n","      <td>61.0</td>\n","      <td>1553.0</td>\n","      <td>473.0</td>\n","      <td>9.0</td>\n","      <td>3.0</td>\n","      <td>2015.0</td>\n","      <td>0.247411</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1879</th>\n","      <td>600000.0</td>\n","      <td>5.0</td>\n","      <td>6.0</td>\n","      <td>7064.0</td>\n","      <td>1995.0</td>\n","      <td>217800.0</td>\n","      <td>1.0</td>\n","      <td>43.0</td>\n","      <td>87.0</td>\n","      <td>66.0</td>\n","      <td>942.0</td>\n","      <td>287.0</td>\n","      <td>8.0</td>\n","      <td>1.0</td>\n","      <td>1999.0</td>\n","      <td>-0.643123</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1880</th>\n","      <td>759900.0</td>\n","      <td>5.0</td>\n","      <td>6.0</td>\n","      <td>7500.0</td>\n","      <td>2006.0</td>\n","      <td>8886.0</td>\n","      <td>1.0</td>\n","      <td>43.0</td>\n","      <td>61.0</td>\n","      <td>51.0</td>\n","      <td>803.0</td>\n","      <td>245.0</td>\n","      <td>5.0</td>\n","      <td>2.0</td>\n","      <td>2009.0</td>\n","      <td>-0.524305</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1881</th>\n","      <td>735000.0</td>\n","      <td>5.0</td>\n","      <td>6.0</td>\n","      <td>7515.0</td>\n","      <td>1958.0</td>\n","      <td>10497.0</td>\n","      <td>1.0</td>\n","      <td>37.0</td>\n","      <td>80.0</td>\n","      <td>86.0</td>\n","      <td>1459.0</td>\n","      <td>444.0</td>\n","      <td>9.0</td>\n","      <td>3.0</td>\n","      <td>2015.0</td>\n","      <td>-0.696683</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      tx_price  ...  property_type_Single-Family\n","1877  385000.0  ...                            1\n","1878  690000.0  ...                            1\n","1879  600000.0  ...                            1\n","1880  759900.0  ...                            1\n","1881  735000.0  ...                            1\n","\n","[5 rows x 32 columns]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"X7kVrx-1WItT","colab_type":"text"},"source":["<hr> \n","\n","<br>\n","\n","## Separate the dataset's features from the target variable\n","\n","**Tasks:**\n","- Print shape of original DataFrame before manipulating the DataFrame\n","- Create a new DataFrame called `X` to contain only the features \n","- Create a new DataFrame called `y` to contain only the labels\n","\n","<br>\n","\n","### Question: \n","Why would you split the data this way?\n","\n"]},{"cell_type":"markdown","metadata":{"id":"haWxJNQqRcnB","colab_type":"text"},"source":["Answer:\n","\n","We will do this to separate the features of the dataset from the target value. For our problem we will set the `tx_price` as the target variable for this machine learning model, because we want to predict the house price based on a selected amount of other features of the dataset.     "]},{"cell_type":"markdown","metadata":{"id":"49c9QRT9RcOn","colab_type":"text"},"source":["<br>\n","\n","### Print Shape Of Original DataFrame\n","We will do this to confirm our manipulations later\n"]},{"cell_type":"code","metadata":{"id":"fEYW_FT5XCW3","colab_type":"code","outputId":"e586ee65-4fbe-43c7-f8ea-cd3f85143e0a","executionInfo":{"status":"ok","timestamp":1574461711896,"user_tz":480,"elapsed":9851,"user":{"displayName":"Matt Hess","photoUrl":"","userId":"00800340733434721825"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["df.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1882, 32)"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"s6mLVXRDXV-E","colab_type":"text"},"source":["### Create A DataFrame Called `X` To Hold All The Features\n","**Note:** `X` is uppercase because it's a 2D array / matrix. A matrix holds multiple rows and more than one column. \n","\n","**Tip:** Consider using the DataFrame's `drop` method to create this new DataFrame\n","\n","\n","#### DataFrame's `drop` method parameters:\n","- `labels` (string or list of strings): index or a  column labels to drop\n","- `axis`  ({0 or ‘index’, 1 or ‘columns’}): default 0; whether to drop labels from the index (0 or ‘index’) or columns (1 or ‘columns’)\n","- `inplace` (bool): default False; If True, do operation inplace and return None.\n","\n"]},{"cell_type":"code","metadata":{"id":"jHCpurRhYoi9","colab_type":"code","colab":{}},"source":["X = df.drop(labels=\"tx_price\", axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WXsA6YQLcB4c","colab_type":"text"},"source":["#### Show Shape of `X` to make sure we created the features DataFrame correctly:\n","\n","It should have 1882 rows and 31 columns"]},{"cell_type":"code","metadata":{"id":"j401z0aVZShG","colab_type":"code","outputId":"3cfbf145-2c0d-4af1-b9ae-8ef9416412b6","executionInfo":{"status":"ok","timestamp":1574461711902,"user_tz":480,"elapsed":9749,"user":{"displayName":"Matt Hess","photoUrl":"","userId":"00800340733434721825"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["X.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1882, 31)"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"a_Ou4ZIpaK-L","colab_type":"text"},"source":["### Print Head Of Features Matrix `X`"]},{"cell_type":"code","metadata":{"id":"FAnSrTc3aNuN","colab_type":"code","outputId":"a7a7c761-8ea5-45a6-c272-67f2302cc2c1","executionInfo":{"status":"ok","timestamp":1574461711906,"user_tz":480,"elapsed":9557,"user":{"displayName":"Matt Hess","photoUrl":"","userId":"00800340733434721825"}},"colab":{"base_uri":"https://localhost:8080/","height":241}},"source":["X.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>beds</th>\n","      <th>baths</th>\n","      <th>sqft</th>\n","      <th>year_built</th>\n","      <th>lot_size</th>\n","      <th>basement</th>\n","      <th>median_age</th>\n","      <th>married</th>\n","      <th>college_grad</th>\n","      <th>property_tax</th>\n","      <th>insurance</th>\n","      <th>median_school</th>\n","      <th>num_schools</th>\n","      <th>tx_year</th>\n","      <th>lifestyle_avg</th>\n","      <th>two_and_two</th>\n","      <th>exterior_walls_Brick</th>\n","      <th>exterior_walls_Brick veneer</th>\n","      <th>exterior_walls_Combination</th>\n","      <th>exterior_walls_Metal</th>\n","      <th>exterior_walls_Other</th>\n","      <th>exterior_walls_Siding (Alum/Vinyl)</th>\n","      <th>exterior_walls_Wood</th>\n","      <th>exterior_walls_missing</th>\n","      <th>roof_Asphalt</th>\n","      <th>roof_Composition Shingle</th>\n","      <th>roof_Other</th>\n","      <th>roof_Shake Shingle</th>\n","      <th>roof_missing</th>\n","      <th>property_type_Apartment / Condo / Townhouse</th>\n","      <th>property_type_Single-Family</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>584.0</td>\n","      <td>2013.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>33.0</td>\n","      <td>65.0</td>\n","      <td>84.0</td>\n","      <td>234.0</td>\n","      <td>81.0</td>\n","      <td>9.0</td>\n","      <td>3.0</td>\n","      <td>2013.0</td>\n","      <td>1.493259</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>612.0</td>\n","      <td>1965.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>39.0</td>\n","      <td>73.0</td>\n","      <td>69.0</td>\n","      <td>169.0</td>\n","      <td>51.0</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>2006.0</td>\n","      <td>0.676598</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>615.0</td>\n","      <td>1963.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>28.0</td>\n","      <td>15.0</td>\n","      <td>86.0</td>\n","      <td>216.0</td>\n","      <td>74.0</td>\n","      <td>8.0</td>\n","      <td>3.0</td>\n","      <td>2012.0</td>\n","      <td>2.298254</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>618.0</td>\n","      <td>2000.0</td>\n","      <td>33541.0</td>\n","      <td>0.0</td>\n","      <td>36.0</td>\n","      <td>25.0</td>\n","      <td>91.0</td>\n","      <td>265.0</td>\n","      <td>92.0</td>\n","      <td>9.0</td>\n","      <td>3.0</td>\n","      <td>2005.0</td>\n","      <td>2.473650</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>634.0</td>\n","      <td>1992.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>37.0</td>\n","      <td>20.0</td>\n","      <td>75.0</td>\n","      <td>88.0</td>\n","      <td>30.0</td>\n","      <td>9.0</td>\n","      <td>3.0</td>\n","      <td>2002.0</td>\n","      <td>1.661371</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   beds  ...  property_type_Single-Family\n","0   1.0  ...                            0\n","1   1.0  ...                            0\n","2   1.0  ...                            0\n","3   1.0  ...                            0\n","4   1.0  ...                            0\n","\n","[5 rows x 31 columns]"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"L5tE9Jx7YkEQ","colab_type":"text"},"source":["### Create A Series Called `y` To Hold All The Labels\n","**Note:** `y` is lowercase because it's Series, meaning it holds multiple rows and only one column per row.\n","\n"]},{"cell_type":"code","metadata":{"id":"lqhOSR5XY1QA","colab_type":"code","colab":{}},"source":["y = df.loc[:, 'tx_price']"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GJaGIwlIZWbJ","colab_type":"text"},"source":["#### Show Shape of `y` to make sure we created the target series correctly:\n","\n","It should have 1882 rows and 1 column\n","\n","**Note:** the shape will print out like this, `(1882,)` which means that is has 1882 rows and 1 column."]},{"cell_type":"code","metadata":{"id":"VYJ3UxBxbz8b","colab_type":"code","outputId":"98278dbd-00a2-4340-e1db-74611fc8ca09","executionInfo":{"status":"ok","timestamp":1574461712023,"user_tz":480,"elapsed":9386,"user":{"displayName":"Matt Hess","photoUrl":"","userId":"00800340733434721825"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["y.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1882,)"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"b4rnN6U0b4ug","colab_type":"text"},"source":["### Print Head Of Label Series `y`"]},{"cell_type":"code","metadata":{"id":"Fk_qjzB2aIho","colab_type":"code","outputId":"9625eb25-a210-4093-ce87-12b36b244455","executionInfo":{"status":"ok","timestamp":1574461712032,"user_tz":480,"elapsed":9272,"user":{"displayName":"Matt Hess","photoUrl":"","userId":"00800340733434721825"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["y.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    295850.0\n","1    216500.0\n","2    279900.0\n","3    379900.0\n","4    340000.0\n","Name: tx_price, dtype: float64"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"EFKzxqnr7uzd","colab_type":"text"},"source":["<hr>\n","<br>\n","\n","## Split Data Into Training And Testing \n","\n","Even though we will perform cross validation in the near future we will still want to split the dataset in to a training and testing set. We do this so that after we find the best estimator, or best fitted model, through utilizing the training data with a specific hyper-parameters values, we can evaluate the model using unseen testing data. This will allow us to understand if the model is overfitting or underfitting. \n","\n","Additional Resources:\n","- [Learn more about overfitting and underfitting](https://github.com/SoftStackFactory/PythonDataScienceHandbook/blob/master/notebooks/05.03-Hyperparameters-and-Model-Validation.ipynb)\n","- [Interested in how to better fit a model?](https://github.com/SoftStackFactory/PythonDataScienceHandbook/blob/master/notebooks/05.04-Feature-Engineering.ipynb)\n","\n","Note: The second resource is very informative about feature engineering, but we specifically want to emphasize the **Derived Features** section. \n","\n","<br>\n","\n","#### Split Data Into Training And Testing Sets Using The `train_test_split` Function\n"," \n","Requirements: \n","- pass in `0.20` as the argument for the `test_size` parameter\n","- pass in `1` as the argument for the `random_state` parameter\n","\n","**Note:** We set the `random_state` parameter to a unique argument value so that when we run this notebook multiple times or using different computers we will recieve the same split of data, which is important for re-running experiments and simulations.\n","\n","[`train_test_split` function documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"nJp6QhTWgE6p"},"source":["### Import `train_test_split` Function From Sklearn's Library"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"q6w6nF17gE6x","colab":{}},"source":["# Helper for splitting training and testing sets\n","from sklearn.model_selection import train_test_split"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2hoPli0Ug2H3","colab_type":"text"},"source":["#### Split Data Into Training And Testing Sets "]},{"cell_type":"code","metadata":{"id":"llMrZ0-e-DX5","colab_type":"code","colab":{}},"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8SdNSJn0SqaG","colab_type":"text"},"source":["<hr>\n","\n","<br>\n","\n","## Cross Validation Recap\n","One disadvantage of using a holdout set, or in other words a static split of data for model validation is that we have lost a portion of our data to the model training. In this case some of the dataset would not contribute to the training of the model, saying we don't use cross validation and stick with splitting the data only once. This is not optimal, and can cause problems – especially if the initial set of training data is small.\n","\n","We will be using cross validation in conjunction to splitting the data using the `train_test_spit` function. We will do this so that we are not training and testing our model's evaluation with the same data, meaning we will eventually want to have our model get scored using unseen data to get a feel for how the model is performing.\n","\n","<br>\n","\n","### Perform A Cross Validation Grid Search \n","A Grid Search Cross Validation is an exhaustive search over specified hyper-parameter values to find the most performant estimator with a specific hyper-parameter set. \n","\n","<br>\n","\n","#### To perform a cross validation grid search on each machine learning algorithm we need to construct the following:\n","- Pipeline object, one for each algorithm \n","- Model hyper-parameter dictionary, one for each algorithm\n","\n","<br>\n","\n","Think of a Pipeline object as production line of transforming `features`, or X DataFrame, before eventually fitting the model. It's important to know that each argument of the pipeline, each transformer object, that will transform the `features` data, must have the following methods implemented:\n","- `fit()` \n","- `fit_transform()`\n","\n","<br>\n","\n","In summary the pipeline will pre-process any `features` data provided to it before fitting the model. We will accomplish this by calling the first parameter's `fit_transform` method. The output of the first object's `fit_transform` method will be passed automatically to the next parameter's `fit_transform` method, and so on. Eventually the output of the last transformer object's `fit_transform` method will be passed to the `fit()` method of the estimator object.\n","\n","#### Rundown of what is conceptually happening:\n","\n","``` python \n","def process_of_pipeline(self, Features, labels):\n","  Features_transformed = Features\n","  for name, transformer_object: in self.steps[:-1]:\n","    Features_transformed = transformer_object.fit_transform(Features_transformed, labels)\n","  estimator = self.steps[-1][1]\n","  estimator.fit(Features_transformed, labels) # The last step is the estimator, fit the model after all the transformation operations are complete\n","```\n","\n","**Note:** `Features` is capitalized because it's a 2D Matrix, which can interchangably be refered to as a DataFrame, or many rows of data each containing multiple columns of data points. While `labels` should be thought of as a Series, because each row only has one column to hold a singular data point.\n","\n","\n","[For a better understanding check out this video](https://www.youtube.com/watch?v=6zk6uQSuXqs)\n","\n","[`GridSearchCV` Class's Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)"]},{"cell_type":"markdown","metadata":{"id":"_e8pPYigmH-_","colab_type":"text"},"source":["<br>\n","\n","### Make a Pipeline\n","\n","We will start by tranforming the `features` matrix data using the `StandardScalar` object's `fit_transform()` method, which will normalize all of the data. We will then pass the transformed `features` matrix into the estimator's `fit` method along with the unmodified `labels` series as parameters.\n"]},{"cell_type":"markdown","metadata":{"id":"t4WrwfFgnw5b","colab_type":"text"},"source":["### Import The Following Libraries From Sklearn's Library\n","- `make_pipeline` function, which will help us create a pipeline object\n","- `LinearRegression` estimator class \n","- `StandardScalar` class, which will normalize the dataset\n","\n","\n","**Remember:** You can tell the difference between a class and a function by the case sensivity\n","\n","- A **class** will be captialized\n","- A **function** will be lowercase"]},{"cell_type":"code","metadata":{"id":"YtuK_fn4jdLC","colab_type":"code","colab":{}},"source":["# Helper for pipelines\n","from sklearn.pipeline import make_pipeline\n","\n","# Model/Estimator\n","from sklearn.linear_model import LinearRegression\n","\n","# Helper for normailizing dataset\n","from sklearn.preprocessing import StandardScaler"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7wNrbht_oAhA","colab_type":"text"},"source":["### Create The Pipeline\n","\n","#### Using the `make_pipeline()` function:\n","- Pass in an instanciated `StandardScalar` object as the first argument\n","- Pass in  an instanciated `LinearRegression` object as the second argument\n","\n","**Note:** Both the `StandardScalar` and `LinearRegression` object will be instanciated with no parameters being passed into their constructor function"]},{"cell_type":"code","metadata":{"id":"spBHCI_0nuvc","colab_type":"code","colab":{}},"source":["pipeline = make_pipeline(StandardScaler(), LinearRegression())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LrSUvCbg-3VJ","colab_type":"text"},"source":["<br>\n","\n","### Make A Hyper-Parameter Dictionary\n","\n","**Hint:**\n","You can find out what hyper-parameters an estimator has by using the pipeline object's `get_params` method. This will return a dictionary. \n","\n","#### Understanding A Dictionary\n","A `dictionary` is an unordered collection of data values, used to store data values like a map, which unlike other Data Types that hold only single value as an element, a dictionary holds `key:value` pair. A dictionary has a `key`, and each key maps to a `unique value`. A dictionary is useful when you are trying locate a specific value based on a key in a collection, opposed to iterating over an array/list to get to find specific value. Picture that you have to cycle through a really long list of items just to find the one you were looking for. Is cycling through all those items really necessary? Technically speaking cycling through a list takes longer time and more computer performance, something we need to be mindful of when working with machine learning with big data. A dictionary allows us to quickly access a value based on a unique key, without having to iterate, or cycle, through all elements in this collection.\n","\n","It's a good time to mention that values of dictionaries can be dictionaries themselves. \n","\n","[Dictionary reference](https://www.geeksforgeeks.org/python-dictionary/)\n","\n","<br>\n","\n","#### To view the pipeline dictionary, print the dictionary using the pipeline object's `get_params` method: \n","```python\n","# pipeline dicitonary\n","pipeline.get_params()\n","```"]},{"cell_type":"code","metadata":{"id":"x-u3-X1S6HKp","colab_type":"code","outputId":"1d1286a5-a251-4f8f-cac6-277ccfb70ce5","executionInfo":{"status":"ok","timestamp":1574461712054,"user_tz":480,"elapsed":8642,"user":{"displayName":"Matt Hess","photoUrl":"","userId":"00800340733434721825"}},"colab":{"base_uri":"https://localhost:8080/","height":272}},"source":["# pipeline dicitonary\n","pipeline.get_params()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'linearregression': LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False),\n"," 'linearregression__copy_X': True,\n"," 'linearregression__fit_intercept': True,\n"," 'linearregression__n_jobs': None,\n"," 'linearregression__normalize': False,\n"," 'memory': None,\n"," 'standardscaler': StandardScaler(copy=True, with_mean=True, with_std=True),\n"," 'standardscaler__copy': True,\n"," 'standardscaler__with_mean': True,\n"," 'standardscaler__with_std': True,\n"," 'steps': [('standardscaler',\n","   StandardScaler(copy=True, with_mean=True, with_std=True)),\n","  ('linearregression',\n","   LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False))],\n"," 'verbose': False}"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"xYHVtIXE6Gun","colab_type":"text"},"source":["\n","#### Invoke the `keys` method on the pipeline dictionary to view all of the pipeline's parameter names\n","\n","```python\n","# pipeline dictionary keys\n","pipeline.get_params().keys()\n","```"]},{"cell_type":"code","metadata":{"id":"WJKIoj6H_XcM","colab_type":"code","outputId":"de7b8d1f-04ed-405d-8cd2-628da486bbc8","executionInfo":{"status":"ok","timestamp":1574461712057,"user_tz":480,"elapsed":8550,"user":{"displayName":"Matt Hess","photoUrl":"","userId":"00800340733434721825"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["pipeline.get_params().keys()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['memory', 'steps', 'verbose', 'standardscaler', 'linearregression', 'standardscaler__copy', 'standardscaler__with_mean', 'standardscaler__with_std', 'linearregression__copy_X', 'linearregression__fit_intercept', 'linearregression__n_jobs', 'linearregression__normalize'])"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"P3h9l_mm7GX0"},"source":["#### To view all the parameter values of the pipeline dictionary, print the invocation of the pipeline dictionary's `values` method:\n","\n","#### We will invoke the `values()` method on the pipeline dictionary to view all the parameter values\n","```python\n","# pipeline dicitonary values\n","pipeline.get_params().values()\n","```"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"CJQ72exM7GX4","outputId":"c9966c8e-0413-4cb2-bc32-361b7654a068","executionInfo":{"status":"ok","timestamp":1574461712181,"user_tz":480,"elapsed":8605,"user":{"displayName":"Matt Hess","photoUrl":"","userId":"00800340733434721825"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["pipeline.get_params().values()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_values([None, [('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('linearregression', LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False))], False, StandardScaler(copy=True, with_mean=True, with_std=True), LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False), True, True, True, True, True, None, False])"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"5GTMQFo__YzL","colab_type":"text"},"source":["### Create A Dictionary To Hold The Different Hyper-Parameters For An Indvidual Estimator\n","\n","For this dictionary:\n","- The `keys` will be a unique estimator hyper-parameter name\n","- The `values` will be arrays, filled with multiple unique values for that specific hyper-parameter\n","\n","#### <span style=\"color:red\"> Important Note: </span>\n","When creating an estimator's hyper-parameter dictionary we need to make sure we are using the pipeline object's dictionary `keys`, not the actual estimator object's dictionary `keys` **i.e.** `LinearRegression()`. \n"," \n","##### Don't Do:\n","\n","<del>\n","\n","```python \n","# Do not use the actual estimator object's keys\n","LinearRegression().get_params().keys()\n","\n","hyper_parameter_dict = {\n","  'fit_intercept': [True, False] # When we fit, an error\n","}\n","```\n","\n","</del>\n","\n","When we perform fitting the model with the following hyper-parameters we will get the following error if we do not use the proper hyper-parameter names:\n","\n","```ValueError: Invalid parameter fit_intercept for estimator Pipeline```\n","\n","This is because we need to use the pipeline's hyper-parameters naming convention instead: \n","\n","```python \n","# Get Pipeline's hyper-parameter options instead due to naming conventions sklearn follows\n","pipeline.get_params().keys()\n","\n","# Use Pipeline's hyper-parameter options instead due to the naming conventions sklearn follows\n","hyper_parameter_dict = {\n","    'linearregression__fit_intercept': [True, False],\n","}\n"]},{"cell_type":"code","metadata":{"id":"2Vzz3WUDnDzs","colab_type":"code","colab":{}},"source":["# Use Pipeline's hyper-parameter options instead due to the naming conventions sklearn follows\n","hyper_parameter_dict = {\n","    'linearregression__fit_intercept': [True, False],\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4pAMAeJAFIWx","colab_type":"text"},"source":["### Perform A Cross Validation Grid Search\n","Now that we have a pipeline and an estimator's hyper-parameter dictionary we can perform a cross validation grid search. This is the preparation step for finding the best estimator with a specific hyper-parameters set.\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"iz2bXzEhz5MA"},"source":["### Import `GridSearchCv` Class From Sklearn's Library"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"syisScoCz5MD","colab":{}},"source":["# Helper for cross-validation\n","from sklearn.model_selection import GridSearchCV"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nFRTy5yAk9A4","colab_type":"text"},"source":["### Build The GridSearchCV Object \n","\n","#### Set the following parameters of the `GridSearchCV` object initialization:\n","- `estimator` parameter will be assigned with the `pipeline` object instance as its argument\n","- `param_grid` parameter will be assigned with the `hyper-parameter-dictionary` instance\n","- `return_train_score` parameter will be assigned with the value `True`\n","- `refit` parameter will be assigned with the value `True`\n","- `n_jobs` parameter will be assigned the value `-1` to use all available cpu power\n","\n","[`GridSearchCV` Class's Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n"]},{"cell_type":"code","metadata":{"id":"YQjlCgNvSqaJ","colab_type":"code","colab":{}},"source":["model = GridSearchCV(pipeline, hyper_parameter_dict, cv=10, n_jobs=-1, return_train_score=True, refit=True) "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iF5743tsPBSn","colab_type":"text"},"source":["<br>\n","\n","#### **Note:** The data type of the model instance is of type GridSearchCv\n","\n","### Use pythons built in `type()` function passing in the model instance to print the type "]},{"cell_type":"code","metadata":{"id":"C01vrP7gsd5p","colab_type":"code","outputId":"86dba9f9-1c05-473b-9724-7404a709dbc8","executionInfo":{"status":"ok","timestamp":1574461712200,"user_tz":480,"elapsed":8439,"user":{"displayName":"Matt Hess","photoUrl":"","userId":"00800340733434721825"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["type(model)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["sklearn.model_selection._search.GridSearchCV"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"P3KR2kLjSqaK","colab_type":"text"},"source":["<br>\n","\n","### Fit Model to Training Data\n","We will now fit an estimator/model. We will do this so that we can use the fitted model to predict the values to which unseen samples belong.\n","This is the training/learning aspect of the model as well as where the pipeline process comes alive!"]},{"cell_type":"code","metadata":{"id":"ealpxp_JSqaL","colab_type":"code","outputId":"25f0e55e-f76c-4f9d-e97d-c618de8cb6a9","executionInfo":{"status":"ok","timestamp":1574461716943,"user_tz":480,"elapsed":13121,"user":{"displayName":"Matt Hess","photoUrl":"","userId":"00800340733434721825"}},"colab":{"base_uri":"https://localhost:8080/","height":343}},"source":["model.fit(X_train, y_train)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n","  DeprecationWarning)\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["GridSearchCV(cv=10, error_score='raise-deprecating',\n","             estimator=Pipeline(memory=None,\n","                                steps=[('standardscaler',\n","                                        StandardScaler(copy=True,\n","                                                       with_mean=True,\n","                                                       with_std=True)),\n","                                       ('linearregression',\n","                                        LinearRegression(copy_X=True,\n","                                                         fit_intercept=True,\n","                                                         n_jobs=None,\n","                                                         normalize=False))],\n","                                verbose=False),\n","             iid='warn', n_jobs=-1,\n","             param_grid={'linearregression__fit_intercept': [True, False]},\n","             pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n","             scoring=None, verbose=0)"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"markdown","metadata":{"id":"lFePfgx3SqaP","colab_type":"text"},"source":["<hr>\n","\n","## Evaluate Model\n","\n","\n","### Highest Performing Model Score\n","We will now find out how well the highest performing model with a specific hyper-parameter value set performs.\n","\n","Use the `GridSearchCV` class's `score` method using the training data. \n","\n","**Note:** The reason why we can train and evaluate with the same data is because we are performing cross validation. \n","\n","This `score` method will use the best estimator's scoring function, in our case the **r^2** score because we are using a LinearRegression estimator.\n","\n","Refrences:\n","- [`GridSearchCV` class's `score` method documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV.score)\n","- [`LinearRegression` class's `score` method documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.score)"]},{"cell_type":"code","metadata":{"id":"BOUJ2lgbsMFd","colab_type":"code","outputId":"654a6c98-2650-408b-c7ed-784ecffe116d","executionInfo":{"status":"ok","timestamp":1574461716947,"user_tz":480,"elapsed":13071,"user":{"displayName":"Matt Hess","photoUrl":"","userId":"00800340733434721825"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["model.score(X_train,y_train) "],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.4541561568926857"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"markdown","metadata":{"id":"WNpVApGSOxsq","colab_type":"text"},"source":["### Mean Performing Score\n","Print the `GridSearchCV` object's `best_score_` attribute to see the mean cross-validated score using the best_estimator\n","\n","Todo: Confirm if `GridSearchCv.best_score_` is using the GridSearchCv.best_estimator to perform the cross validation."]},{"cell_type":"code","metadata":{"id":"oiaboVTeNISD","colab_type":"code","outputId":"547a2419-579a-477e-8ea2-2a295876ff7b","executionInfo":{"status":"ok","timestamp":1574461716949,"user_tz":480,"elapsed":13008,"user":{"displayName":"Matt Hess","photoUrl":"","userId":"00800340733434721825"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["model.best_score_"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.404873450982989"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"markdown","metadata":{"id":"TDdl1tpsOVW6","colab_type":"text"},"source":["# Store the Best Fitted Estimator\n","\n","Assign a new variable named `best_estimator` with the GridSearchCV object's `best_estimator_` attribute\n","\n","**Remember:** `best_estimator_` is actually a `Pipeline` object"]},{"cell_type":"code","metadata":{"id":"W9LU4AuDS5Ce","colab_type":"code","colab":{}},"source":["best_estimator = model.best_estimator_"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2UNWgM3iTBwF","colab_type":"text"},"source":["<br>\n","\n","### Question\n","What can we conclude from looking at the best_estimator's set hyperparameters?\n","\n","### Answer:\n","The most performant model will have the value `True` for the `fit_intercept` hyper-parameter.\n","\n","<br>\n","\n","#### Important note: \n","The `best_estimator`'s hyper-parameter values were specifically chosen because of the data it was trained on. If you manipulate the data and then re-fit the model we might notice different values.  \n"]},{"cell_type":"markdown","metadata":{"id":"__w0VzMiUvk3","colab_type":"text"},"source":["<hr>\n","\n","<br>\n","\n","### Evaluate the best performing model using the testing data\n","\n","**Remember:** `best_estimator_` is actually a `Pipeline` object. \n","\n","Once you have access to the pipeline object(s) then use the `score` method passing in X_test and y_test as arguments.\n","\n","<br>\n","\n","### <span style=\"color:red\">**It's really important to always understand the type of object you are working with, use that object's documentation to guide you in the right direction** </span>"]},{"cell_type":"code","metadata":{"id":"eXB8pDS4U7rm","colab_type":"code","outputId":"dbf6422e-771e-4406-f851-df96d955b4a7","executionInfo":{"status":"ok","timestamp":1574461716958,"user_tz":480,"elapsed":12914,"user":{"displayName":"Matt Hess","photoUrl":"","userId":"00800340733434721825"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["best_estimator.score(X_test, y_test)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.46674832991442416"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"markdown","metadata":{"id":"ztkuhwGwslI7","colab_type":"text"},"source":["<hr>\n","\n","# Make Predictions\n","Now that we have a fitted model we can now make a prediction using the testing set.\n","\n","Make predictions using the first five observations of the testing data."]},{"cell_type":"code","metadata":{"id":"lH4jyt6Ft_hu","colab_type":"code","outputId":"5420361a-d4fb-40bc-ccfb-86b67cc0ffcf","executionInfo":{"status":"ok","timestamp":1574461716961,"user_tz":480,"elapsed":12825,"user":{"displayName":"Matt Hess","photoUrl":"","userId":"00800340733434721825"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["for prediction in model.predict(X_test[:5]):\n","  print(prediction)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["426755.0020805967\n","656771.0020805968\n","493411.0020805967\n","584163.0020805968\n","386659.0020805967\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5eG-iHrGuUu2","colab_type":"text"},"source":["### Compare Predictions To The True Values"]},{"cell_type":"code","metadata":{"id":"0GleCOR5uEJj","colab_type":"code","outputId":"8635b8bb-b3ac-4d53-901b-16dcb41f971d","executionInfo":{"status":"ok","timestamp":1574461716964,"user_tz":480,"elapsed":12686,"user":{"displayName":"Matt Hess","photoUrl":"","userId":"00800340733434721825"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["for actual_value in y_test[:5]:\n","  print(actual_value)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["360000.0\n","765000.0\n","333250.0\n","590000.0\n","369900.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9su0AV1nu_Bl","colab_type":"text"},"source":["#### What else can you do to boost the model performance?"]},{"cell_type":"markdown","metadata":{"id":"yWoX5aWwvaUM","colab_type":"text"},"source":["Iterate:\n","- Exploratory analysis\n","- Data cleaning \n","- Feature engineering\n","- Get More Data"]},{"cell_type":"code","metadata":{"id":"fq3gjVrJv3Pl","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}
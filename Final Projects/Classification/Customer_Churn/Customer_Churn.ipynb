{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"Customer_Churn.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"4IdMwe1P9JvI","colab_type":"text"},"source":["# Predicting Customer Churn"]},{"cell_type":"markdown","metadata":{"id":"8VMnrGjq9JvL","colab_type":"text"},"source":["<hr>\n","\n","**Problem Description:**\n","\n","This dataset contains several observations, each represents a single customer and its features. We want to predict whether a customer will \"churn\" or not, in this instance to \"churn\" means to no longer require the services of the business in question, to \"NOT\" churn is to stay on as a customer.\n","\n","Perform EDA to get an idea as to what the underlying patterns may be between whether a customer \"churned\" or not. Remember, our squishy human brains can only contend with a few variables at a time, so the ultimate objective is to build a model that codifies the patterns present accross SEVERAL variables, to build a roboust predictor of customer Churn.\n","\n","<hr>\n","\n","<br>\n","\n","## Overview\n","\n","<br>\n","\n","**Import Dataset**\n","\n","**Exploratory Data Analysis:**\n","  - Identify Feature Data Types and Values Counts\n","  - Analyze Distributions Using Various types of Plots \n","  - Identify What Each Feature's Distribution Is Describing About The Target Variable\n","    - [Click Link To Explore If There Is Possiblility To Transform This Distribution To A Normal Distribution](https://medium.com/ai-techsystems/gaussian-distribution-why-is-it-important-in-data-science-and-machine-learning-9adbe0e5f8ac)\n","  - Analyze Correlations Using A Heatmap\n","\n","<br>\n","\n","**Data Cleaning:**\n","  - Handle Nan Values Possibly Using Imputation\n","  - Handle Duplicates\n","  - Fix Structural Errors\n","    - Typos\n","    - Possibly Bin Similar Values\n","  - Remove Unused Variables\n","  - Handle Outliers\n","  - [Click Link To Learn About Normalizing Features Using Z-Score](https://lazyprogrammer.me/what-the-hell-is-a-z-score/)\n","  \n","<br>\n","\n","**Feature Engineering:**\n","  - Discretization For Numerical Values\n","  - Bin Nominal Categorical Values\n","    - After Binning One Hot Encode These Features\n","  - Encode Ordinal Categorical Values As A Indicator Variable\n","    - Don't One Hot Encode Oridinal Categorical Values to preserve more information\n","\n","<br>\n","\n","**View Distributions After Data Cleaning and Feature Engineering** \n","\n","<br>\n","\n","**Preparation of Data:**\n","  - Splitting of Data Into Train and Test Sets\n","  - Separate the dataset's features from target variable\n","  - Split data into training and testing sets\n","\n","<br>\n","\n","**Modeling:**\n","- Make a pipeline\n","- Make a hyper-parameter dictionary\n","- Perform a cross validaton grid search using the training set, and setting the following parameters of the `GridSearchCV` object initialization:\n","  - `estimator` parameter will be assigned with the `pipeline` object instance as its argument\n","  - `param_grid` parameter will be assigned with the `hyper-parameter-dictionary` instance\n","- Identify best performing model using the training set with specific hyper-parameters set\n","- Create a confusion matrix to understand the model's performance\n","- Evaluate the best performing model using the testing data\n","\n","<br>\n","\n","**REPEAT WITH NEW FEATURE ENGINEERING TECHNIQUES** \n","\n","**This is more of an iterative process!** \n","\n","You may build a model only to find you're accuracy is low, which will require you to go back and engineer new features or maybe preform some more EDA to ensure that you've selected the most important features, given the problem at hand. Look the article discussing [PCA](https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60) to get an understanding of how each feature is impacting the model.\n","\n","<br>\n","\n","**Resources:**\n","- [Distributions and Correlations Exploratory Data Analysis, Along With Dataset Cleaning and Preparation](https://www.neuraldesigner.com/learning/tutorials/data-set)\n","- [Outliers](https://www.neuraldesigner.com/blog/3_methods_to_deal_with_outliers)\n","- [Tips On Feature Engineering](https://github.com/SoftStackFactory/PythonDataScienceHandbook/blob/master/notebooks/05.04-Feature-Engineering.ipynb)\n","- [Identifying Feature Importances with PCA](https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60)\n","- [Data Science Handbook](https://github.com/SoftStackFactory/PythonDataScienceHandbook/)\n"]},{"cell_type":"markdown","metadata":{"id":"f_dzCahn9De2","colab_type":"text"},"source":["\n","<hr>\n"]},{"cell_type":"markdown","metadata":{"id":"7X0G9cuk89Qp","colab_type":"text"},"source":["# Good Luck!"]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"vg_sales.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"elztXXOA4o_l","colab_type":"text"},"source":["# Final Project: Video Game Sales Prediction"]},{"cell_type":"markdown","metadata":{"id":"-qMJ6j7o4o_n","colab_type":"text"},"source":["**Problem Description:**\n","\n","This dataset contains several observations, each represents a single Video Game, with several features including a game title, metacritic rating for the game, publisher, developer, etc...\n","\n","**Warning**: Not every game has a matching metacritic review, as some of the older games existed before the time of metacritic. So you will have to weigh your options with this missing data, drop, fill in, etc...\n","\n","Your taks is to predict the ```Global Sales``` column, you must perform some EDA to find which features are most strongly correlated with the ```Global_sales``` figure, clean and preprocess the data, split up dataset into test and train, model the data against several models, and evaluate model performance to find the most performant model.\n","\n","\n","\n","<hr>\n","\n","<br>\n","\n","## Overview\n","\n","<br>\n","\n","**Import Dataset**\n","\n","**Exploratory Data Analysis:**\n","  - Identify Feature Data Types and Values Counts\n","  - Analyze Distributions Using Various types of Plots \n","  - Identify What Each Feature's Distribution Is Describing About The Target Variable\n","    - [Click Link To Explore If There Is Possiblility To Transform This Distribution To A Normal Distribution](https://medium.com/ai-techsystems/gaussian-distribution-why-is-it-important-in-data-science-and-machine-learning-9adbe0e5f8ac)\n","  - Analyze Correlations Using A Heatmap\n","\n","<br>\n","\n","**Data Cleaning:**\n","  - Handle Nan Values Possibly Using Imputation\n","  - Handle Duplicates\n","  - Fix Structural Errors\n","    - Typos\n","    - Possibly Bin Similar Values\n","  - Remove Unused Variables\n","  - Handle Outliers\n","  - [Click Link To Learn About Normalizing Features Using Z-Score](https://lazyprogrammer.me/what-the-hell-is-a-z-score/)\n","  \n","<br>\n","\n","**Feature Engineering:**\n","  - Discretization For Numerical Values\n","  - Bin Nominal Categorical Values\n","    - After Binning One Hot Encode These Features\n","  - Encode Ordinal Categorical Values As A Indicator Variable, \n","    - Don't One Hot Encode Oridinal Categorical Values to preserve more information\n","\n","<br>\n","\n","**View Distributions After Data Cleaning and Feature Engineering** \n","\n","<br>\n","\n","**Preparation of Data:**\n","  - Splitting of Data Into Train and Test Sets\n","  - Separate the dataset's features from target variable\n","  - Split data into training and testing sets\n","\n","<br>\n","\n","**Modeling:**\n","- Make a pipeline\n","- Make a hyper-parameter dictionary\n","- Perform a cross validaton grid search using the training set, and setting the following parameters of the `GridSearchCV` object initialization:\n","  - `estimator` parameter will be assigned with the `pipeline` object instance as its argument\n","  - `param_grid` parameter will be assigned with the `hyper-parameter-dictionary` instance\n","- Identify best performing model using the training set with specific hyper-parameters set\n","- Evaluate the best performing model using the testing data\n","\n","<br>\n","\n","**REPEAT WITH NEW FEATURE ENGINEERING TECHNIQUES** \n","\n","**This is more of an iterative process!** \n","\n","You may build a model only to find you're accuracy is low, which will require you to go back and engineer new features or maybe preform some more EDA to ensure that you've selected the most important features, given the problem at hand. Look the article discussing [how to evaluate a linear regression model]((https://www.ritchieng.com/machine-learning-evaluate-linear-regression-model/)as well as the article discussing [PCA](https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60) to get an understanding of how each feature is impacting the model.\n","\n","<br>\n","\n","**Resources:**\n","- [Distributions and Correlations Exploratory Data Analysis, Along With Dataset Cleaning and Preparation](https://www.neuraldesigner.com/learning/tutorials/data-set)\n","- [Outliers](https://www.neuraldesigner.com/blog/3_methods_to_deal_with_outliers)\n","- [Tips On Feature Engineering](https://github.com/SoftStackFactory/PythonDataScienceHandbook/blob/master/notebooks/05.04-Feature-Engineering.ipynb)\n","- [How To Evaluate A Linear Regression Model](https://www.ritchieng.com/machine-learning-evaluate-linear-regression-model/) \n","- [Identifying Feature Importances with PCA](https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60)\n","- [Data Science Handbook](https://github.com/SoftStackFactory/PythonDataScienceHandbook/)\n"]},{"cell_type":"markdown","metadata":{"id":"Fld-OIlA4o_o","colab_type":"text"},"source":["<hr>\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"wTT1j6ki4pAV","colab_type":"text"},"source":["# Good Luck!"]}]}
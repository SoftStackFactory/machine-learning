{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"House_Prices.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"R74uYUmuk5U2","colab_type":"text"},"source":["# DS201 Final Project: Predicting House Prices"]},{"cell_type":"markdown","metadata":{"id":"Bg57Q_MBk5U5","colab_type":"text"},"source":["<hr>\n","\n","**Problem Description:**\n","\n","This dataset contains several observations, each represents a single Home sold in the city of Ames, Iowa, with 79 Explanatory features describing every aspect of the home. From Square Area to Type of transaction. I have provided a data dictionary in the data files, I have shown how to open this file in Python, below. \n","\n","Your task is to predict the ```SalePrice``` column, you must perform some EDA to find which features are most strongly correlated with the ```Global_sales``` figure, clean and preprocess the data, model the data against several models, and evaluate model performance to find the most performant model.\n","\n","It is up to you how you want to handle test.csv, you can append it to the bottom of train, or you can simply run it through the same transformations that you ran on train. \n","\n","You could also ignore it and split train.csv itself into a test and train set.\n","\n","<hr>\n","\n","<br>\n","\n","## Overview\n","\n","<br>\n","\n","**Import Dataset**\n","\n","**Exploratory Data Analysis:**\n","  - Identify Feature Data Types and Values Counts\n","  - Analyze Distributions Using Various types of Plots \n","  - Identify What Each Feature's Distribution Is Describing About The Target Variable\n","    - [Click Link To Explore If There Is Possiblility To Transform This Distribution To A Normal Distribution](https://medium.com/ai-techsystems/gaussian-distribution-why-is-it-important-in-data-science-and-machine-learning-9adbe0e5f8ac)\n","  - Analyze Correlations Using A Heatmap\n","\n","<br>\n","\n","**Data Cleaning:**\n","  - Handle Nan Values Possibly Using Imputation\n","  - Handle Duplicates\n","  - Fix Structural Errors\n","    - Typos\n","    - Possibly Bin Similar Values\n","  - Remove Unused Variables\n","  - Handle Outliers\n","  - [Click Link To Learn About Normalizing Features Using Z-Score](https://lazyprogrammer.me/what-the-hell-is-a-z-score/)\n","  \n","<br>\n","\n","**Feature Engineering:**\n","  - Discretization For Numerical Values\n","  - Bin Nominal Categorical Values\n","    - After Binning One Hot Encode These Features\n","  - Encode Ordinal Categorical Values As A Indicator Variable, \n","    - Don't One Hot Encode Oridinal Categorical Values to preserve more information\n","\n","<br>\n","\n","**View Distributions After Data Cleaning and Feature Engineering** \n","\n","<br>\n","\n","**Preparation of Data:**\n","  - Splitting of Data Into Train and Test Sets\n","  - Separate the dataset's features from target variable\n","  - Split data into training and testing sets\n","\n","<br>\n","\n","**Modeling:**\n","- Make a pipeline\n","- Make a hyper-parameter dictionary\n","- Perform a cross validaton grid search using the training set, and setting the following parameters of the `GridSearchCV` object initialization:\n","  - `estimator` parameter will be assigned with the `pipeline` object instance as its argument\n","  - `param_grid` parameter will be assigned with the `hyper-parameter-dictionary` instance\n","- Identify best performing model using the training set with specific hyper-parameters set\n","- Evaluate the best performing model using the testing data\n","\n","<br>\n","\n","**REPEAT WITH NEW FEATURE ENGINEERING TECHNIQUES** \n","\n","**This is more of an iterative process!** \n","\n","You may build a model only to find you're accuracy is low, which will require you to go back and engineer new features or maybe preform some more EDA to ensure that you've selected the most important features, given the problem at hand. Look the article discussing [how to evaluate a linear regression model]((https://www.ritchieng.com/machine-learning-evaluate-linear-regression-model/)as well as the article discussing [PCA](https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60) to get an understanding of how each feature is impacting the model.\n","\n","<br>\n","\n","**Resources:**\n","- [Distributions and Correlations Exploratory Data Analysis, Along With Dataset Cleaning and Preparation](https://www.neuraldesigner.com/learning/tutorials/data-set)\n","- [Outliers](https://www.neuraldesigner.com/blog/3_methods_to_deal_with_outliers)\n","- [Tips On Feature Engineering](https://github.com/SoftStackFactory/PythonDataScienceHandbook/blob/master/notebooks/05.04-Feature-Engineering.ipynb)\n","- [How To Evaluate A Linear Regression Model](https://www.ritchieng.com/machine-learning-evaluate-linear-regression-model/) \n","- [Identifying Feature Importances with PCA](https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60)\n","- [Data Science Handbook](https://github.com/SoftStackFactory/PythonDataScienceHandbook/)\n"]},{"cell_type":"markdown","metadata":{"id":"MtGcv7Gak5U6","colab_type":"text"},"source":["<hr>\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"n3wNC_Dak5Vk","colab_type":"text"},"source":["# Good Luck!"]}]}